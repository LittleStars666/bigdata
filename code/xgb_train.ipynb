{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "C55B539C8B454C029EB15BB082A876D6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The klab-autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext klab-autotime\n",
      "time: 1.26 ms\n"
     ]
    }
   ],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0E2CA2CA8A0D4D468461CE96DC8AD70D",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USER=kesci\n",
      "time: 1.05 ms\n"
     ]
    }
   ],
   "source": [
    "%env USER=kesci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D5A45B5EEF494D71873D85D593B60D0F",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.76 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import catboost as ctb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "30D19660CEB343BFA5CF4434986CEAE9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.7 ms\n"
     ]
    }
   ],
   "source": [
    "def get_rate(data):\n",
    "    data['type0_rate'] = data['act_type0_count']/(data['act_day_count']+0.00001)\n",
    "    data['type1_rate'] = data['act_type1_count']/(data['act_day_count']+0.00001)\n",
    "    data['type2_rate'] = data['act_type2_count']/(data['act_day_count']+0.00001)\n",
    "    data['type3_rate'] = data['act_type3_count']/(data['act_day_count']+0.00001)\n",
    "    \n",
    "    data['page0_rate'] = data['act_page0_count']/(data['act_day_count']+0.00001)\n",
    "    data['page1_rate'] = data['act_page1_count']/(data['act_day_count']+0.00001)\n",
    "    data['page2_rate'] = data['act_page2_count']/(data['act_day_count']+0.00001)\n",
    "    data['page3_rate'] = data['act_page3_count']/(data['act_day_count']+0.00001)\n",
    "    data['page4_rate'] = data['act_page4_count']/(data['act_day_count']+0.00001)\n",
    "    \n",
    "    act_mean = data['act_day_count'].mean()\n",
    "    data['is_have_act'] = data['act_day_count'].apply(lambda x:0 if x<=0 else (1 if x < act_mean else 2))\n",
    "    video_mean = data['video_day_count'].mean()\n",
    "    data['is_have_video'] = data['video_day_count'].apply(lambda x:0 if x<=0 else (1 if x < video_mean else 2))\n",
    "    type0_mean = data['act_type0_count'].mean()\n",
    "    data['is_have_type0'] = data['act_type0_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < type0_mean else 2))\n",
    "    type1_mean = data['act_type1_count'].mean()\n",
    "    data['is_have_type1'] = data['act_type1_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < type1_mean else 2))\n",
    "    type2_mean = data['act_type2_count'].mean()\n",
    "    data['is_have_type2'] = data['act_type2_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < type2_mean else 2))\n",
    "    \n",
    "    act_mean = data['act_day_count'].mean()\n",
    "    data['act_day_count1'] = data['act_day_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < act_mean else 2))\n",
    "    \n",
    "    act_max = data['act_day_max'].mean()\n",
    "    data['act_day_max1'] = data['act_day_max'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < act_max else 2))\n",
    "    data['act_day_max2'] = data['act_day_max'].fillna(0).apply(lambda x:0 if x<=0 else 1)\n",
    "    # page2_mean = data['act_page2_count'].mean()\n",
    "    # data['is_have_page2'] = data['act_page2_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < page2_mean else 2))\n",
    "    data['every_video_rate'] = data['act_day_count']/(data['act_video_nums']+0.00001)\n",
    "    data['every_video_author_rate'] = data['act_author_counts']/(data['act_video_nums']+0.00001)\n",
    "    # data['author_act_rate'] =data['act_author_counts']/data['act_day_count']\n",
    "    \n",
    "    data['act_daily_max_rate'] = data['act_daily_max']/(data['act_day_count']+0.00001)\n",
    "    data['act_single_video_rate'] = data['single_video_max']/(data['act_day_count']+0.00001)\n",
    "    # data['app_daily_max_rate'] = data['app_daily_max']/(data['app_day_count']+0.00001)\n",
    "    \n",
    "    # data['every_page_type_rate'] = data['every_page_type_max']/data['act_daily_max']\n",
    "    # data['every_video_type_rate'] = data['every_video_type_max']/data['act_daily_max']\n",
    "    \n",
    "    data['app_act_rate'] = data['app_day_std']/(data['act_last1days_counts']+0.00001)\n",
    "    data['app_video_type_rate'] = data['app_day_max']/(data['every_video_type_mean']+0.00001)\n",
    "    data['app_video_type_sub'] = data['app_day_max']-data['every_video_type_mean']\n",
    "    \n",
    "    data['page1_2_rate'] = data['page2_rate']/(data['page1_rate']+0.00001)\n",
    "    data['app_std_mul'] = data['app_day_std']*data['page1_rate']\n",
    "    data['act_last1_mul'] = data['act_last1days_counts']*data['page1_rate']\n",
    "    data['act_last1_std_mul'] = (data['act_last1days_counts']+data['app_day_std'])*data['page1_rate']\n",
    "    \n",
    "    data['app_diff_rate'] = data['app_day_diff_std']/(data['app_day_std']+0.00001)\n",
    "    data['app_page1_rate'] = data['app_day_diff_std']*data['page1_rate']\n",
    "    data['page1_type2_rate'] = data['page1_rate']*data['type2_rate']\n",
    "    \n",
    "    data['app_std_div_mean'] = data['app_day_std']/(data['app_day_mean']+0.00001)\n",
    "    data['act_std_div_mean'] = data['act_day_std']/(data['act_day_mean']+0.00001)\n",
    "    data['video_std_div_mean'] = data['video_day_std']/(data['video_day_mean']+0.00001)\n",
    "    \n",
    "    data['register_day'] = data['register_day'] + 1\n",
    "    # data['register_day'] = data['register_day'].apply(lambda x:16 if x>16 else x)\n",
    "    \n",
    "    data['device_type1'] = data['device_type'].apply(lambda x:0 if x< 85 else (1 if x<170 else 2))\n",
    "    \n",
    "    data['act_continue_std_div_mean'] = data['act_continue_std']/(data['act_continue_mean']+0.00001)\n",
    "    data['app_continue_std_div_mean'] = data['app_continue_std']/(data['app_continue_mean']+0.00001)\n",
    "    \n",
    "    data['app_div_reg'] = data['app_day_count']/data['register_day']\n",
    "    data['act_div_reg'] = data['act_day_count']/data['register_day']\n",
    "    data['video_div_reg'] = data['video_day_count']/data['register_day']\n",
    "    data['author_div_reg'] = data['act_author_count']/data['register_day']\n",
    "    data['act_video_div_reg'] = data['act_video_nums']/data['register_day']\n",
    "    \n",
    "    data['act_last1days_div_reg'] = data['act_last1days_counts']/data['register_day']\n",
    "    \n",
    "    # data['act_author_q3_q1'] = data['act_author_q3']-(data['act_author_q1']+0.00001)\n",
    "    # data['act_author_q2'] = data['act_author_q1']*2\n",
    "    # data['act_author_q4'] = data['act_author_q3']+data['act_author_q1']\n",
    "    \n",
    "    # data['every_video_type_std_mean'] = data['every_video_type_std']+data['every_video_type_mean']\n",
    "    data['device_exp_reg'] = data['device_type']/(data['register_type']+0.00001)\n",
    "    \n",
    "    data['video_max_min'] = data['video_day_max']-data['video_day_min']\n",
    "    data['app_max_min'] = data['app_day_max']-data['app_day_min']\n",
    "    data['act_max_min'] = data['act_day_max']-data['act_day_min']\n",
    "    \n",
    "    data['act_app_day_exp'] = data['app_day_max']/(data['act_day_max']+0.00001)\n",
    "    data['video_app_day_exp'] = data['app_day_max']/(data['video_day_max']+0.00001)\n",
    "    \n",
    "    data['app_day_diff_std_mean'] = data['app_day_diff_std']/(data['app_day_diff_mean']+0.00001)\n",
    "    data['app_day_std_mean'] = data['app_day_std']/(data['app_day_mean']+0.00001)\n",
    "    \n",
    "    data['act_app_day_mul'] = data['app_day_max']+data['act_day_max']\n",
    "    data['video_app_day_mul'] = data['app_day_max']+data['video_day_max']\n",
    "    data['video_act_day_mul'] = data['act_day_max']+data['video_day_max']\n",
    "    \n",
    "    data['author_rate'] = data['act_author_count']/data['act_author_counts']\n",
    "    \n",
    "    data['act_author_rate'] = data['act_author_counts']/data['act_day_count']\n",
    "    data['app_author_rate'] = data['act_author_counts']/data['app_day_count']\n",
    "    data['video_author_rate'] = data['act_author_counts']/data['video_day_count']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0734798C3EA54547BA5163D798BBDCFA",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.7 ms\n"
     ]
    }
   ],
   "source": [
    "def xgb_train(train,test):\n",
    "    # print(train.columns.values.tolist())\n",
    "    # print(len(train.columns.values.tolist()))\n",
    "    \n",
    "    test_feature = test.drop(\"user_id\",1)\n",
    "    \n",
    "    l = pd.DataFrame()\n",
    "    l['label'] = train['label']\n",
    "    l = l.groupby('label',as_index=False)['label'].agg({'label_count':'count'})\n",
    "    print(l)\n",
    "    \n",
    "    label = train['label']      \n",
    "    train.drop(['user_id','label'], inplace=True, axis=1)\n",
    "    \n",
    "    used_feature =[\n",
    "        \n",
    "        'act_author_count', 'act_author_counts', 'act_continue_mean', \n",
    "        'act_continue_std', 'act_day_mean', 'act_day_min',  \n",
    "        'act_last1days_counts', 'act_last2days_counts', 'act_last3days_counts', 'act_last4days_counts', \n",
    "        'act_last5days_counts', 'act_last6days_counts', 'act_last7days_counts', 'act_last_gap', 'act_page0_count', \n",
    "        'act_page1_count', 'act_page2_count', 'act_page3_count', 'act_page4_count', 'act_type0_count', \n",
    "        'act_type1_count', 'act_type2_count', 'act_type3_count', \n",
    "        'app_author_rate',  'app_continue_std', 'app_day_count', \n",
    "        'app_day_diff_max', 'app_day_diff_mean', 'app_day_diff_std',\n",
    "        'app_day_max', 'app_day_mean', 'app_day_std',\n",
    "        'app_last3days_counts', 'app_last4days_counts', 'app_last5days_counts', 'app_last6days_counts', \n",
    "        'app_last7days_counts', 'app_last_gap', 'author_rate', 'device_type', 'page0_rate', 'page1_rate',\n",
    "        'page2_rate', 'register_day', 'register_type', 'type0_rate', \n",
    "        'type1_rate', 'type2_rate', 'video_author_rate', \n",
    "        'video_day_mean', 'video_day_std',\n",
    "        'video_last7days_counts','act_single_video_rate', 'act_app_day_exp', 'video_app_day_exp','every_video_type_mean', \n",
    "        'every_page_type_mean', 'act_daily_max', 'act_video_nums', 'single_video_max',\n",
    "        'app_act_rate','app_video_type_rate','app_video_type_sub',\n",
    "        \n",
    "        'device_exp_reg',\n",
    "        'video_app_day_mul','act_app_day_mul','video_act_day_mul',\n",
    "        'is_have_act','is_have_video','is_have_type2','is_have_type1','is_have_type0',\n",
    "        'app_act_rate','act_daily_max_rate',\n",
    "        'app_day_skew','app_day_kurt','act_day_skew','act_day_kurt',\n",
    "        'act_nums','video_nums','act_day_sum','app_day_sum',\n",
    "        'device_type_register_rate',\n",
    "    ]\n",
    "    t = train[used_feature]\n",
    "    text_use_feature = test[used_feature]\n",
    "    \n",
    "    N = 5\n",
    "    skf = StratifiedKFold(n_splits=N,shuffle=True,random_state=6666)\n",
    "    \n",
    "    xx_cv = []\n",
    "    xx_pre = []\n",
    "    train_value = t.values\n",
    "    label = label.values\n",
    "    test_feature = text_use_feature.values\n",
    "    \n",
    "    train_features = t.columns.tolist()\n",
    "    df = pd.DataFrame(train_features, columns=['feature'])\n",
    "    df1 = pd.DataFrame(train_features, columns=['feature'])\n",
    "    imp1 = []\n",
    "    i = 0\n",
    "    for train_in,test_in in skf.split(train_value,label):\n",
    "        print(\"折数\",i+1)\n",
    "        X_train,X_test,y_train,y_test = train_value[train_in],train_value[test_in],label[train_in],label[test_in]\n",
    "        \n",
    "        xgb_val = xgb.DMatrix(X_test,label=y_test)\n",
    "        xgb_train = xgb.DMatrix(X_train,label=y_train)\n",
    "        xgb_test = xgb.DMatrix(test_feature)\n",
    "    \n",
    "        params = {\n",
    "            'boosting_type': 'gbtree',\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'gamma':0.1,\n",
    "            'max_depth': 5,\n",
    "            'min_child_weight': 3,\n",
    "            'eta': 0.05,\n",
    "            'silent': 0,\n",
    "            'subsample': 0.8,\n",
    "            'alpha': 0.6,  \n",
    "            'lambda': 0.001,  # 越小l2正则程度越高\n",
    "            'colsample_bytree': 0.5,\n",
    "        }\n",
    "        \n",
    "        plst = list(params.items())\n",
    "        num_rounds = 20000  # 迭代次数\n",
    "        watchlist = [(xgb_train, 'train'), (xgb_val, 'val')]\n",
    "        \n",
    "        print('开始训练......')\n",
    "        gbm = xgb.train(\n",
    "           plst,\n",
    "           xgb_train,\n",
    "           num_rounds,\n",
    "           watchlist,\n",
    "           early_stopping_rounds=300,\n",
    "           verbose_eval=300\n",
    "       )\n",
    "        X_test=xgb.DMatrix(X_test)\n",
    "        y_pred = gbm.predict(X_test)\n",
    "        # y_test=pd.Series(y_test)\n",
    "        xx_cv.append(roc_auc_score(y_test,y_pred))\n",
    "        xx_pre.append(gbm.predict(xgb_test))\n",
    "        i = i+1\n",
    "    \n",
    "    print('xx_cv',np.mean(xx_cv))\n",
    "    s = 0\n",
    "    for i in xx_pre:\n",
    "        s = s + i\n",
    "    s = s/N\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['user_id'] = test['user_id']\n",
    "    df_result['probability'] = list(s)\n",
    "    time_date = time.strftime('%Y-%m-%d',time.localtime(time.time()))\n",
    "    df_result.to_csv('/home/kesci/bigdata/result/xgb_%s_%s.csv'%(str(time_date),str(np.mean(xx_cv)).split('.')[1]),sep=',',index=False,header=None)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DCF58985A0B94274800B186382F9E33B",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  label_count\n",
      "0      0       339024\n",
      "1      1       339220\n",
      "折数 1\n",
      "开始训练......\n",
      "[0]\ttrain-auc:0.877938\tval-auc:0.876982\n",
      "Multiple eval metrics have been passed: 'val-auc' will be used for early stopping.\n",
      "\n",
      "Will train until val-auc hasn't improved in 300 rounds.\n",
      "[300]\ttrain-auc:0.891767\tval-auc:0.888887\n",
      "[600]\ttrain-auc:0.894536\tval-auc:0.889195\n",
      "[900]\ttrain-auc:0.896808\tval-auc:0.889216\n",
      "Stopping. Best iteration:\n",
      "[821]\ttrain-auc:0.896243\tval-auc:0.88923\n",
      "\n",
      "折数 2\n",
      "开始训练......\n",
      "[0]\ttrain-auc:0.878105\tval-auc:0.877913\n",
      "Multiple eval metrics have been passed: 'val-auc' will be used for early stopping.\n",
      "\n",
      "Will train until val-auc hasn't improved in 300 rounds.\n",
      "[300]\ttrain-auc:0.891609\tval-auc:0.889423\n",
      "[600]\ttrain-auc:0.894244\tval-auc:0.889698\n",
      "[900]\ttrain-auc:0.896635\tval-auc:0.88973\n",
      "Stopping. Best iteration:\n",
      "[759]\ttrain-auc:0.895521\tval-auc:0.889753\n",
      "\n",
      "折数 3\n",
      "开始训练......\n",
      "[0]\ttrain-auc:0.878086\tval-auc:0.877857\n",
      "Multiple eval metrics have been passed: 'val-auc' will be used for early stopping.\n",
      "\n",
      "Will train until val-auc hasn't improved in 300 rounds.\n",
      "[300]\ttrain-auc:0.891779\tval-auc:0.889028\n",
      "[600]\ttrain-auc:0.894458\tval-auc:0.889298\n",
      "[900]\ttrain-auc:0.896795\tval-auc:0.889373\n",
      "Stopping. Best iteration:\n",
      "[872]\ttrain-auc:0.89658\tval-auc:0.889387\n",
      "\n",
      "折数 4\n",
      "开始训练......\n",
      "[0]\ttrain-auc:0.878178\tval-auc:0.877763\n",
      "Multiple eval metrics have been passed: 'val-auc' will be used for early stopping.\n",
      "\n",
      "Will train until val-auc hasn't improved in 300 rounds.\n",
      "[300]\ttrain-auc:0.891772\tval-auc:0.88913\n",
      "[600]\ttrain-auc:0.894442\tval-auc:0.889336\n",
      "[900]\ttrain-auc:0.896824\tval-auc:0.889426\n",
      "[1200]\ttrain-auc:0.899041\tval-auc:0.889393\n",
      "Stopping. Best iteration:\n",
      "[991]\ttrain-auc:0.897519\tval-auc:0.88944\n",
      "\n",
      "折数 5\n",
      "开始训练......\n",
      "[0]\ttrain-auc:0.879702\tval-auc:0.879453\n",
      "Multiple eval metrics have been passed: 'val-auc' will be used for early stopping.\n",
      "\n",
      "Will train until val-auc hasn't improved in 300 rounds.\n",
      "[300]\ttrain-auc:0.891745\tval-auc:0.888743\n",
      "[600]\ttrain-auc:0.894386\tval-auc:0.888954\n",
      "[900]\ttrain-auc:0.896728\tval-auc:0.888984\n",
      "Stopping. Best iteration:\n",
      "[775]\ttrain-auc:0.895766\tval-auc:0.888995\n",
      "\n",
      "xx_cv 0.889327200627967\n",
      "time: 26min 32s\n"
     ]
    }
   ],
   "source": [
    "f1 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck1_feature.csv\")\n",
    "f1 = get_rate(f1)\n",
    "\n",
    "f2 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck2_feature.csv\")\n",
    "f2 = get_rate(f2)\n",
    "\n",
    "f = pd.concat([f1,f2],axis=0,ignore_index=True,join='outer')\n",
    "\n",
    "f3 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck3_feature.csv\")\n",
    "f3 = get_rate(f3)\n",
    "s = xgb_train(f,f3)\n",
    "test_label = f2['label']\n",
    "print(\"线下验证--》\",roc_auc_score(test_label,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "B78165643BD549A9AC008E4A37DB467A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.1 ms\n"
     ]
    }
   ],
   "source": [
    "def cat_train(train,test):\n",
    "    # print(train.columns.values.tolist())\n",
    "    # print(len(train.columns.values.tolist()))\n",
    "    \n",
    "    test_feature = test.drop(\"user_id\",1)\n",
    "    \n",
    "    l = pd.DataFrame()\n",
    "    l['label'] = train['label']\n",
    "    l = l.groupby('label',as_index=False)['label'].agg({'label_count':'count'})\n",
    "    print(l)\n",
    "    \n",
    "    label = train['label']      \n",
    "    train.drop(['user_id','label'], inplace=True, axis=1)\n",
    "    \n",
    "    used_feature =[\n",
    "        'act_author_count', 'act_author_counts', 'act_continue_mean', \n",
    "        'act_continue_std', 'act_day_mean', 'act_day_min',  \n",
    "        'act_last1days_counts', 'act_last2days_counts', 'act_last3days_counts', 'act_last4days_counts', \n",
    "        'act_last5days_counts', 'act_last6days_counts', 'act_last7days_counts', 'act_last_gap', 'act_page0_count', \n",
    "        'act_page1_count', 'act_page2_count', 'act_page3_count', 'act_page4_count', 'act_type0_count', \n",
    "        'act_type1_count', 'act_type2_count', 'act_type3_count', \n",
    "        'app_author_rate',  'app_continue_std', 'app_day_count', \n",
    "        'app_day_diff_max', 'app_day_diff_mean', 'app_day_diff_std',\n",
    "        'app_day_max', 'app_day_mean', 'app_day_std',\n",
    "        'app_last3days_counts', 'app_last4days_counts', 'app_last5days_counts', 'app_last6days_counts', \n",
    "        'app_last7days_counts', 'app_last_gap', 'author_rate', 'device_type', 'page0_rate', 'page1_rate',\n",
    "        'page2_rate', 'register_day', 'register_type', 'type0_rate', \n",
    "        'type1_rate', 'type2_rate', 'video_author_rate', \n",
    "        'video_day_mean', 'video_day_std',\n",
    "        'video_last7days_counts','act_single_video_rate', 'act_app_day_exp', 'video_app_day_exp','every_video_type_mean', \n",
    "        'every_page_type_mean', 'act_daily_max', 'act_video_nums', 'single_video_max',\n",
    "        'app_act_rate','app_video_type_rate','app_video_type_sub',\n",
    "        \n",
    "        'device_exp_reg',\n",
    "        'video_app_day_mul','act_app_day_mul','video_act_day_mul',\n",
    "        'is_have_act','is_have_video','is_have_type2','is_have_type1','is_have_type0',\n",
    "        'app_act_rate','act_daily_max_rate',\n",
    "        'app_day_skew','app_day_kurt','act_day_skew','act_day_kurt',\n",
    "        'act_nums','video_nums','act_day_sum','app_day_sum',\n",
    "        'device_type_register_rate',\n",
    "        ]\n",
    "    t = train[used_feature]\n",
    "    text_use_feature = test[used_feature]\n",
    "    \n",
    "    N = 5\n",
    "    skf = StratifiedKFold(n_splits=N,shuffle=True,random_state=6666)\n",
    "    \n",
    "    xx_cv = []\n",
    "    xx_pre = []\n",
    "    train_value = t.values\n",
    "    label = label.values\n",
    "    test_feature = text_use_feature.values\n",
    "    \n",
    "    i = 0\n",
    "    for train_in,test_in in skf.split(train_value,label):\n",
    "        print(\"折数\",i+1)\n",
    "        X_train,X_test,y_train,y_test = train_value[train_in],train_value[test_in],label[train_in],label[test_in]\n",
    "        \n",
    "        dtrain = ctb.Pool(X_train,y_train)\n",
    "        dvalid = ctb.Pool(X_test,y_test)  \n",
    "\n",
    "        model = ctb.CatBoostClassifier(\n",
    "            thread_count = 8, \n",
    "            iterations = 10000,\n",
    "            depth=4, \n",
    "            learning_rate=0.05,\n",
    "            eval_metric='AUC',\n",
    "            verbose=300,\n",
    "            od_type = 'Iter',\n",
    "            # l2_leaf_reg=0.001,\n",
    "            od_wait = 150)\n",
    "            \n",
    "        model.fit(dtrain, eval_set=dvalid)\n",
    "        y_pred = model.predict_proba(X_test)[:,1]\n",
    "        # print(\"pred-->\",model.predict_proba(X_test))\n",
    "        xx_cv.append(roc_auc_score(y_test,y_pred))\n",
    "        xx_pre.append(model.predict_proba(test_feature)[:,1])\n",
    "        i = i+1\n",
    "        \n",
    "    print('xx_cv',np.mean(xx_cv))\n",
    "    s = 0\n",
    "    for i in xx_pre:\n",
    "        s = s + i\n",
    "    s = s/N\n",
    "    \n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['user_id'] = test['user_id']\n",
    "    df_result['probability'] = list(s)\n",
    "    time_date = time.strftime('%Y-%m-%d',time.localtime(time.time()))\n",
    "    df_result.to_csv('/home/kesci/bigdata/result/cat_%s_%s.csv'%(str(time_date),str(np.mean(xx_cv)).split('.')[1]),sep=',',index=False,header=None)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "D3A6B5F617D346F894E9CFF86D825D07",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  label_count\n",
      "0      0       339024\n",
      "1      1       339220\n",
      "折数 1\n",
      "0:\tlearn: 0.8622341\ttest: 0.8614123\tbest: 0.8614123 (0)\ttotal: 234ms\tremaining: 38m 57s\n",
      "300:\tlearn: 0.8882045\ttest: 0.8873415\tbest: 0.8873415 (300)\ttotal: 1m 32s\tremaining: 49m 44s\n",
      "600:\tlearn: 0.8893587\ttest: 0.8881873\tbest: 0.8881873 (600)\ttotal: 3m 9s\tremaining: 49m 25s\n",
      "900:\tlearn: 0.8900376\ttest: 0.8885240\tbest: 0.8885240 (900)\ttotal: 4m 44s\tremaining: 47m 48s\n",
      "1200:\tlearn: 0.8905604\ttest: 0.8886962\tbest: 0.8886962 (1200)\ttotal: 6m 18s\tremaining: 46m 14s\n",
      "1500:\tlearn: 0.8909998\ttest: 0.8887819\tbest: 0.8887853 (1482)\ttotal: 7m 53s\tremaining: 44m 41s\n",
      "1800:\tlearn: 0.8913896\ttest: 0.8888439\tbest: 0.8888459 (1769)\ttotal: 9m 28s\tremaining: 43m 6s\n",
      "2100:\tlearn: 0.8917736\ttest: 0.8888955\tbest: 0.8888972 (2027)\ttotal: 11m 2s\tremaining: 41m 30s\n",
      "2400:\tlearn: 0.8921212\ttest: 0.8889255\tbest: 0.8889269 (2371)\ttotal: 12m 36s\tremaining: 39m 54s\n",
      "2700:\tlearn: 0.8924478\ttest: 0.8889614\tbest: 0.8889626 (2691)\ttotal: 14m 10s\tremaining: 38m 19s\n",
      "3000:\tlearn: 0.8927569\ttest: 0.8889855\tbest: 0.8889873 (2996)\ttotal: 15m 45s\tremaining: 36m 44s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.888988155\n",
      "bestIteration = 3005\n",
      "\n",
      "Shrink model to first 3006 iterations.\n",
      "折数 2\n",
      "0:\tlearn: 0.8620544\ttest: 0.8621313\tbest: 0.8621313 (0)\ttotal: 235ms\tremaining: 39m 10s\n",
      "300:\tlearn: 0.8880579\ttest: 0.8880308\tbest: 0.8880308 (300)\ttotal: 1m 33s\tremaining: 50m 4s\n",
      "600:\tlearn: 0.8892926\ttest: 0.8888448\tbest: 0.8888448 (600)\ttotal: 3m 7s\tremaining: 48m 53s\n",
      "900:\tlearn: 0.8899363\ttest: 0.8891162\tbest: 0.8891162 (900)\ttotal: 4m 44s\tremaining: 47m 53s\n",
      "1200:\tlearn: 0.8904456\ttest: 0.8892735\tbest: 0.8892739 (1199)\ttotal: 6m 19s\tremaining: 46m 19s\n",
      "1500:\tlearn: 0.8908765\ttest: 0.8893534\tbest: 0.8893547 (1494)\ttotal: 7m 54s\tremaining: 44m 44s\n",
      "1800:\tlearn: 0.8912815\ttest: 0.8894232\tbest: 0.8894239 (1792)\ttotal: 9m 29s\tremaining: 43m 10s\n",
      "2100:\tlearn: 0.8916538\ttest: 0.8894698\tbest: 0.8894698 (2100)\ttotal: 11m 4s\tremaining: 41m 38s\n",
      "2400:\tlearn: 0.8920093\ttest: 0.8895021\tbest: 0.8895021 (2400)\ttotal: 12m 41s\tremaining: 40m 10s\n",
      "2700:\tlearn: 0.8923401\ttest: 0.8895196\tbest: 0.8895213 (2623)\ttotal: 14m 16s\tremaining: 38m 34s\n",
      "3000:\tlearn: 0.8926580\ttest: 0.8895482\tbest: 0.8895492 (2994)\ttotal: 15m 51s\tremaining: 36m 59s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.8895507472\n",
      "bestIteration = 3009\n",
      "\n",
      "Shrink model to first 3010 iterations.\n",
      "折数 3\n",
      "0:\tlearn: 0.8620455\ttest: 0.8614883\tbest: 0.8614883 (0)\ttotal: 233ms\tremaining: 38m 49s\n",
      "300:\tlearn: 0.8881350\ttest: 0.8877129\tbest: 0.8877129 (300)\ttotal: 1m 32s\tremaining: 49m 48s\n",
      "600:\tlearn: 0.8893558\ttest: 0.8884762\tbest: 0.8884762 (600)\ttotal: 3m 7s\tremaining: 48m 55s\n",
      "900:\tlearn: 0.8900097\ttest: 0.8887330\tbest: 0.8887330 (900)\ttotal: 4m 42s\tremaining: 47m 32s\n",
      "1200:\tlearn: 0.8905087\ttest: 0.8888669\tbest: 0.8888680 (1199)\ttotal: 6m 18s\tremaining: 46m 12s\n",
      "1500:\tlearn: 0.8909476\ttest: 0.8889855\tbest: 0.8889855 (1500)\ttotal: 7m 56s\tremaining: 44m 55s\n",
      "1800:\tlearn: 0.8913537\ttest: 0.8890592\tbest: 0.8890598 (1799)\ttotal: 9m 30s\tremaining: 43m 17s\n",
      "2100:\tlearn: 0.8917279\ttest: 0.8891181\tbest: 0.8891181 (2100)\ttotal: 11m 5s\tremaining: 41m 42s\n",
      "2400:\tlearn: 0.8920849\ttest: 0.8891565\tbest: 0.8891585 (2393)\ttotal: 12m 40s\tremaining: 40m 5s\n",
      "2700:\tlearn: 0.8924184\ttest: 0.8891857\tbest: 0.8891859 (2699)\ttotal: 14m 16s\tremaining: 38m 34s\n",
      "3000:\tlearn: 0.8927345\ttest: 0.8892118\tbest: 0.8892123 (2990)\ttotal: 15m 52s\tremaining: 37m\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.8892205169\n",
      "bestIteration = 3044\n",
      "\n",
      "Shrink model to first 3045 iterations.\n",
      "折数 4\n",
      "0:\tlearn: 0.8619629\ttest: 0.8622036\tbest: 0.8622036 (0)\ttotal: 240ms\tremaining: 39m 58s\n",
      "300:\tlearn: 0.8881177\ttest: 0.8877121\tbest: 0.8877121 (300)\ttotal: 1m 34s\tremaining: 50m 49s\n",
      "600:\tlearn: 0.8893275\ttest: 0.8884209\tbest: 0.8884215 (598)\ttotal: 3m 9s\tremaining: 49m 20s\n",
      "900:\tlearn: 0.8900239\ttest: 0.8887091\tbest: 0.8887091 (900)\ttotal: 4m 43s\tremaining: 47m 43s\n",
      "1200:\tlearn: 0.8905303\ttest: 0.8888372\tbest: 0.8888372 (1200)\ttotal: 6m 18s\tremaining: 46m 10s\n",
      "1500:\tlearn: 0.8909703\ttest: 0.8889270\tbest: 0.8889282 (1486)\ttotal: 7m 52s\tremaining: 44m 36s\n",
      "1800:\tlearn: 0.8913715\ttest: 0.8889793\tbest: 0.8889805 (1796)\ttotal: 9m 28s\tremaining: 43m 8s\n",
      "2100:\tlearn: 0.8917457\ttest: 0.8890088\tbest: 0.8890106 (2089)\ttotal: 11m 4s\tremaining: 41m 38s\n",
      "2400:\tlearn: 0.8920931\ttest: 0.8890574\tbest: 0.8890583 (2389)\ttotal: 12m 39s\tremaining: 40m 2s\n",
      "2700:\tlearn: 0.8924312\ttest: 0.8890686\tbest: 0.8890723 (2691)\ttotal: 14m 13s\tremaining: 38m 27s\n",
      "3000:\tlearn: 0.8927525\ttest: 0.8890819\tbest: 0.8890829 (2990)\ttotal: 15m 50s\tremaining: 36m 56s\n",
      "3300:\tlearn: 0.8930692\ttest: 0.8890968\tbest: 0.8890997 (3242)\ttotal: 17m 26s\tremaining: 35m 24s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.889099749\n",
      "bestIteration = 3242\n",
      "\n",
      "Shrink model to first 3243 iterations.\n",
      "折数 5\n",
      "0:\tlearn: 0.8618062\ttest: 0.8628058\tbest: 0.8628058 (0)\ttotal: 235ms\tremaining: 39m 7s\n",
      "300:\tlearn: 0.8881924\ttest: 0.8875163\tbest: 0.8875163 (300)\ttotal: 1m 32s\tremaining: 49m 42s\n",
      "600:\tlearn: 0.8893845\ttest: 0.8882686\tbest: 0.8882686 (600)\ttotal: 3m 8s\tremaining: 49m 11s\n",
      "900:\tlearn: 0.8901035\ttest: 0.8885744\tbest: 0.8885746 (896)\ttotal: 4m 43s\tremaining: 47m 43s\n",
      "1200:\tlearn: 0.8906103\ttest: 0.8887146\tbest: 0.8887147 (1198)\ttotal: 6m 19s\tremaining: 46m 17s\n",
      "1500:\tlearn: 0.8910760\ttest: 0.8888270\tbest: 0.8888283 (1496)\ttotal: 7m 53s\tremaining: 44m 43s\n",
      "1800:\tlearn: 0.8914779\ttest: 0.8888603\tbest: 0.8888628 (1790)\ttotal: 9m 28s\tremaining: 43m 9s\n",
      "2100:\tlearn: 0.8918731\ttest: 0.8889125\tbest: 0.8889132 (2097)\ttotal: 11m 3s\tremaining: 41m 33s\n",
      "2400:\tlearn: 0.8922175\ttest: 0.8889451\tbest: 0.8889456 (2396)\ttotal: 12m 40s\tremaining: 40m 6s\n",
      "2700:\tlearn: 0.8925629\ttest: 0.8889735\tbest: 0.8889739 (2699)\ttotal: 14m 16s\tremaining: 38m 34s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.888976148\n",
      "bestIteration = 2707\n",
      "\n",
      "Shrink model to first 2708 iterations.\n",
      "xx_cv 0.8891670632228583\n",
      "time: 1h 25min 12s\n"
     ]
    }
   ],
   "source": [
    "f1 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck1_feature.csv\")\n",
    "f1 = get_rate(f1)\n",
    "\n",
    "f2 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck2_feature.csv\")\n",
    "f2 = get_rate(f2)\n",
    "\n",
    "f = pd.concat([f1,f2],axis=0,ignore_index=True,join='outer')\n",
    "\n",
    "f3 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck3_feature.csv\")\n",
    "f3 = get_rate(f3)\n",
    "s = cat_train(f,f3)\n",
    "# test_label = f2['label']\n",
    "# print(\"线下验证--》\",roc_auc_score(test_label,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "705BA92DD4FE4E2E8219E80FE03E5746",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.2 ms\n"
     ]
    }
   ],
   "source": [
    "def xgb_lr_train(train,test):\n",
    "    # print(train.columns.values.tolist())\n",
    "    # print(len(train.columns.values.tolist()))\n",
    "    \n",
    "    col =[\n",
    "        'act_author_count', 'act_author_counts', 'act_continue_mean', \n",
    "        'act_continue_std', 'act_day_mean', 'act_day_min',  \n",
    "        'act_last1days_counts', 'act_last2days_counts', 'act_last3days_counts', 'act_last4days_counts', \n",
    "        'act_last5days_counts', 'act_last6days_counts', 'act_last7days_counts', 'act_last_gap', 'act_page0_count', \n",
    "        'act_page1_count', 'act_page2_count', 'act_page3_count', 'act_page4_count', 'act_type0_count', \n",
    "        'act_type1_count', 'act_type2_count', 'act_type3_count', \n",
    "        'app_author_rate',  'app_continue_std', 'app_day_count', \n",
    "        'app_day_diff_max', 'app_day_diff_mean', 'app_day_diff_std',\n",
    "        'app_day_max', 'app_day_mean', 'app_day_std',\n",
    "        'app_last3days_counts', 'app_last4days_counts', 'app_last5days_counts', 'app_last6days_counts', \n",
    "        'app_last7days_counts', 'app_last_gap', 'author_rate', 'device_type', 'page0_rate', 'page1_rate',\n",
    "        'page2_rate', 'register_day', 'register_type', 'type0_rate', \n",
    "        'type1_rate', 'type2_rate', 'video_author_rate', \n",
    "        'video_day_mean', 'video_day_std',\n",
    "        'video_last7days_counts','act_single_video_rate', 'act_app_day_exp', 'video_app_day_exp','every_video_type_mean', \n",
    "        'every_page_type_mean', 'act_daily_max', 'act_video_nums', 'single_video_max',\n",
    "        'app_act_rate','app_video_type_rate','app_video_type_sub',\n",
    "        \n",
    "        'device_exp_reg',\n",
    "        'video_app_day_mul','act_app_day_mul','video_act_day_mul',\n",
    "        'is_have_act','is_have_video','is_have_type2','is_have_type1','is_have_type0',\n",
    "        'act_daily_max_rate',\n",
    "        'app_day_skew','app_day_kurt','act_day_skew','act_day_kurt',\n",
    "        'act_nums','video_nums','act_day_sum','app_day_sum',\n",
    "        'device_type_register_rate',\n",
    "        ]\n",
    "    X_train = train[col]\n",
    "    y_train = train['label'].values\n",
    "    X_test = test[col]\n",
    "    # y_test = test['label'].values\n",
    "    \n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        nthread=8,# cpu 线程数 默认最大\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        seed=6666,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        min_child_weight=3,\n",
    "        colsample_bytree=.9,\n",
    "        gamma = 0.05,\n",
    "        reg_alpha=0.7,\n",
    "        reg_lambda=0.05,\n",
    "        silent=False,\n",
    "        eval_metric='auc'\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict_proba(X_test)[:,1]\n",
    "    # xgb_test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "    # print('xgboost test auc: %.5f' % xgb_test_auc)\n",
    "            \n",
    "    # xgboost编码原有特征\n",
    "    X_train_leaves = model.apply(X_train)\n",
    "    X_test_leaves = model.apply(X_test)\n",
    "    \n",
    "    # 合并编码后的训练数据和测试数据\n",
    "    All_leaves = np.concatenate((X_train_leaves, X_test_leaves), axis=0)\n",
    "    All_leaves = All_leaves.astype(np.int32)\n",
    "    \n",
    "    # 对所有特征进行ont-hot编码\n",
    "    xgbenc = OneHotEncoder()\n",
    "    X_trans = xgbenc.fit_transform(All_leaves)\n",
    "    \n",
    "    (train_rows, cols) = X_train_leaves.shape\n",
    "    \n",
    "    # 定义LR模型\n",
    "    lr = LogisticRegression(n_jobs=8,max_iter=3000,C = 0.01,\n",
    "          verbose=1)\n",
    "    # lr对xgboost特征编码后的样本模型训练\n",
    "    lr.fit(X_trans[:train_rows, :], y_train)\n",
    "    # 预测及AUC评测\n",
    "    y_pred_xgblr1 = lr.predict_proba(X_trans[train_rows:, :])[:, 1]\n",
    "    # xgb_lr_auc1 = roc_auc_score(y_test, y_pred_xgblr1)\n",
    "    # print('基于Xgb特征编码后的LR AUC: %.5f' % xgb_lr_auc1)\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['user_id'] = test['user_id']\n",
    "    df_result['probability'] = y_pred_xgblr1\n",
    "    time_date = time.strftime('%Y-%m-%d',time.localtime(time.time()))\n",
    "    df_result.to_csv('/home/kesci/bigdata/result/xgb_lr_%s.csv' %(str(time_date)), sep=',',index=False,header=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B7585944954F41009F45AFBD0F178F64",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "f1 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck1_feature.csv\")\n",
    "f1 = get_rate(f1)\n",
    "\n",
    "f2 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck2_feature.csv\")\n",
    "f2 = get_rate(f2)\n",
    "\n",
    "f = pd.concat([f1,f2],axis=0,ignore_index=True,join='outer')\n",
    "\n",
    "f3 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck3_feature.csv\")\n",
    "f3 = get_rate(f3)\n",
    "xgb_lr_train(f,f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76B0A3127C4E4ED1878CAF9251C2134D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
