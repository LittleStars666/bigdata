{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EA86E61DA14D49729387B04574CA2C53",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "89414481FC8441239A634C88679F1F40",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F8F22397F4F84C728108ACB773BC8791",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.4 ms\n"
     ]
    }
   ],
   "source": [
    "def rate_fun(data):\n",
    "    data['type3_rate'] = data['act_type3_count']/(data['act_day_count']+0.00001)\n",
    "    data['type2_rate'] = data['act_type2_count']/(data['act_day_count']+0.00001)\n",
    "    data['type1_rate'] = data['act_type1_count']/(data['act_day_count']+0.00001)\n",
    "    data['type0_rate'] = data['act_type0_count']/(data['act_day_count']+0.00001)\n",
    "    \n",
    "    data['register_day'] = data['register_day'] + 1\n",
    "    \n",
    "    data['page0_rate'] = data['act_page0_count']/(data['act_day_count']+0.00001)\n",
    "    data['page1_rate'] = data['act_page1_count']/(data['act_day_count']+0.00001)\n",
    "    data['page2_rate'] = data['act_page2_count']/(data['act_day_count']+0.00001)\n",
    "    data['page3_rate'] = data['act_page3_count']/(data['act_day_count']+0.00001)\n",
    "    data['page4_rate'] = data['act_page4_count']/(data['act_day_count']+0.00001)\n",
    "    \n",
    "    data['dev1'] = data.groupby('device_type')['device_type'].transform('count').astype(np.uint16)\n",
    "    \n",
    "    data['device_exp_reg'] = data['device_type']/(data['register_type']+0.00001)\n",
    "    \n",
    "    data['device_exp_app'] = data['device_type']/(data['app_day_max']+0.00001)\n",
    "    \n",
    "    data['app_max_exp_std'] = data['app_day_max']/(data['app_day_std']+0.00001)\n",
    "    data['app_max_add_std'] = data['app_day_max']+data['app_day_std']\n",
    "    \n",
    "    data['act_13_14_count'] = data['act_day13_count']+data['act_day14_count']\n",
    "    \n",
    "    data['app_max_mul_std'] = data['app_day_max']*data['app_day_std']\n",
    "    data['app_act_mul_skew'] = data['app_day_skew']*data['act_day_skew']\n",
    "    \n",
    "    data['act_day15_rate'] = data['act_day15_count']/(data['act_day_count']+0.00001)\n",
    "    data['act_day14_rate'] = data['act_day14_count']/(data['act_day_count']+0.00001)\n",
    "    data['act_day13_rate'] = data['act_day13_count']/(data['act_day_count']+0.00001)\n",
    "    \n",
    "    data['type0_day15_rate'] = data['act_type0_count']/(data['act_day15_count']+0.00001)\n",
    "    data['page0_day15_rate'] = data['act_page0_count']/(data['act_day15_count']+0.00001)\n",
    "    \n",
    "    data['type1_day15_rate'] = data['act_type1_count']/(data['act_day15_count']+0.00001)\n",
    "    data['page1_day15_rate'] = data['act_page1_count']/(data['act_day15_count']+0.00001)\n",
    "    \n",
    "    data['type2_day15_rate'] = data['act_type2_count']/(data['act_day15_count']+0.00001)\n",
    "    data['page2_day15_rate'] = data['act_page2_count']/(data['act_day15_count']+0.00001)\n",
    "    \n",
    "    data['is_have_act_video'] = data['act_day_count']+data['video_day_count']\n",
    "    act_video_mean = data['is_have_act_video'].mean()\n",
    "    data['is_have_act_video'] = data['is_have_act_video'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < act_video_mean else 2))\n",
    "    \n",
    "    data['is_have_type012'] = data['act_type1_count']+data['act_type0_count']+data['act_type2_count']\n",
    "    type012_mean = data['is_have_type012'].mean()\n",
    "    data['is_have_type012_1'] = data['is_have_type012'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < type012_mean else 2))\n",
    "    \n",
    "    data['is_have_type12'] = data['act_type1_count']+data['act_type2_count']\n",
    "    type12_mean = data['is_have_type12'].mean()\n",
    "    data['is_have_type12_1'] = data['is_have_type12'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < type12_mean else 2))\n",
    "    \n",
    "    data['is_have_act_type12'] = data['act_type1_count']+data['act_type2_count']+data['act_day_count']\n",
    "    have_act_type12_mean = data['is_have_act_type12'].mean()\n",
    "    data['is_have_act_type12_1'] = data['is_have_act_type12'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < have_act_type12_mean else 2))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "874D664963614B14A3F0C524417BB274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.1 ms\n"
     ]
    }
   ],
   "source": [
    "def train_data(train,test):\n",
    "\n",
    "    # print(train.columns.values.tolist())\n",
    "    # print(len(train.columns.values.tolist()))\n",
    "    \n",
    "    test_feature = test.drop(\"user_id\",1)\n",
    "    \n",
    "    print(\"----->\")\n",
    "    l = pd.DataFrame()\n",
    "    l['label'] = train['label']\n",
    "    l = l.groupby('label',as_index=False)['label'].agg({'label_count':'count'})\n",
    "    print(l)\n",
    "    \n",
    "    label = train['label']      \n",
    "    train.drop(['user_id','label'], inplace=True, axis=1)\n",
    "    \n",
    "    used_feature =[\n",
    "        'register_day', 'register_type', 'device_type', 'act_day_count', 'act_day_skew', 'act_day_std',\n",
    "        'act_day_mean', 'act_day_max', 'act_continue_mean', 'act_day11_count', 'act_day10_count', 'act_day13_count',\n",
    "        'act_day4_count', 'act_day3_count', 'act_day12_count', 'act_day8_count', 'act_day5_count', 'act_day7_count',\n",
    "        'act_day9_count', 'act_day6_count', 'act_day15_count', 'act_day2_count', 'act_day14_count', 'act_page0_count', \n",
    "        'act_page2_count', 'act_page3_count', 'act_page1_count', 'act_page4_count', 'act_type0_count', 'act_type1_count', \n",
    "        'act_type3_count', 'act_type2_count', 'act_author_std', \n",
    "        'act_author_mean', 'act_author_count', 'act_last_gap', 'act_daily_mean', 'act_daily_std', 'act_video_nums', \n",
    "        'single_video_mean', 'single_video_std', 'every_video_type_std', 'every_video_type_mean', \n",
    "        'video_day_std', 'video_day_mean', 'video_day_count', 'video_day_max', 'video_day_skew', \n",
    "        'video_daily_mean', 'video_daily_std', 'app_day_count', 'app_day_mean', 'app_day_std', \n",
    "        'app_day_skew', 'app_day_max', 'app_day_diff_max', 'app_day_diff_mean', 'app_day_diff_std',\n",
    "        'app_continue_mean', 'app_last_gap',  \n",
    "        'device_exp_reg','type0_rate','type1_rate','type2_rate','page0_rate','page2_rate',\n",
    "        'device_type_register_rate',\n",
    "        'is_have_act_video','is_have_type012','is_have_type12','is_have_act_type12','is_have_act_type12_1',\n",
    "        'is_have_type12_1','is_have_type012_1','dev2'\n",
    "    ]\n",
    "    t = train[used_feature]\n",
    "    text_use_feature = test[used_feature]\n",
    "    \n",
    "    N = 5\n",
    "    skf = StratifiedKFold(n_splits=N,shuffle=True,random_state=6666)\n",
    "    \n",
    "    xx_cv = []\n",
    "    xx_pre = []\n",
    "    train_value = t.values\n",
    "    label = label.values\n",
    "    test_feature = text_use_feature.values\n",
    "    \n",
    "    train_features = t.columns.tolist()\n",
    "    df = pd.DataFrame(train_features, columns=['feature'])\n",
    "    df1 = pd.DataFrame(train_features, columns=['feature'])\n",
    "    i = 0\n",
    "    for train_in,test_in in skf.split(train_value,label):\n",
    "        print(\"折数\",i+1)\n",
    "        X_train,X_test,y_train,y_test = train_value[train_in],train_value[test_in],label[train_in],label[test_in]\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "    \n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': {'auc'},\n",
    "            'num_leaves': 13,\n",
    "            'max_depth': 4,\n",
    "            'min_data_in_leaf': 300,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.85,\n",
    "            'bagging_freq': 5,\n",
    "            'lambda_l1': 0.6,  \n",
    "            'lambda_l2': 0.7,  # 越小l2正则程度越高\n",
    "            'min_gain_to_split': 0.2,\n",
    "            'verbose': 5,\n",
    "            'max_bin': 90\n",
    "        }\n",
    "        print('开始训练......')\n",
    "        gbm = lgb.train(\n",
    "           params,\n",
    "           lgb_train,\n",
    "           num_boost_round=40000,\n",
    "           valid_sets=lgb_eval,\n",
    "           early_stopping_rounds=300,\n",
    "           verbose_eval=300\n",
    "       )\n",
    "    \n",
    "    # gbm.save_model('/Users/maomao/Desktop/bulesky/data/train_test/lgb_model.txt')\n",
    "        y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "        xx_cv.append(roc_auc_score(y_test,y_pred))\n",
    "        xx_pre.append(gbm.predict(test_feature, num_iteration=gbm.best_iteration))\n",
    "        df['importance'+str(i)]=list(gbm.feature_importance())  \n",
    "        i = i+1\n",
    "        df1['importance'] = df.mean(1)    \n",
    "        df1 = df1.sort_values(by='importance',ascending=False)    \n",
    "    print (df1)\n",
    "    df1.to_csv('/home/kesci/bigdata/result/feature_importance.csv',index=False)\n",
    "    print (list(df1['feature']))\n",
    "    \n",
    "    print('xx_cv',np.mean(xx_cv))\n",
    "    s = 0\n",
    "    for i in xx_pre:\n",
    "        s = s + i\n",
    "    s = s/N\n",
    "    test_label = test['label']\n",
    "    print(\"线下验证分数\",roc_auc_score(test_label,s))\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['user_id'] = test['user_id']\n",
    "    df_result['probability'] = list(s)\n",
    "    time_date = time.strftime('%Y-%m-%d',time.localtime(time.time()))\n",
    "    df_result.to_csv('/home/kesci/bigdata/result/lgb_%s_%s.csv'%(str(time_date),str(np.mean(xx_cv)).split('.')[1]),sep=',',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "3BD97C92041F42CB8DD361EB6B170844",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->\n",
      "   label  label_count\n",
      "0      0       127952\n",
      "1      1       136540\n",
      "折数 1\n",
      "开始训练......\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[300]\tvalid_0's auc: 0.884299\n",
      "[600]\tvalid_0's auc: 0.884093\n",
      "Early stopping, best iteration is:\n",
      "[335]\tvalid_0's auc: 0.884337\n",
      "折数 2\n",
      "开始训练......\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[300]\tvalid_0's auc: 0.881991\n",
      "[600]\tvalid_0's auc: 0.881899\n",
      "Early stopping, best iteration is:\n",
      "[347]\tvalid_0's auc: 0.882021\n",
      "折数 3\n",
      "开始训练......\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[300]\tvalid_0's auc: 0.882517\n",
      "[600]\tvalid_0's auc: 0.882474\n",
      "Early stopping, best iteration is:\n",
      "[410]\tvalid_0's auc: 0.88261\n",
      "折数 4\n",
      "开始训练......\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[300]\tvalid_0's auc: 0.881612\n",
      "Early stopping, best iteration is:\n",
      "[294]\tvalid_0's auc: 0.88162\n",
      "折数 5\n",
      "开始训练......\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[300]\tvalid_0's auc: 0.880246\n",
      "[600]\tvalid_0's auc: 0.88023\n",
      "Early stopping, best iteration is:\n",
      "[469]\tvalid_0's auc: 0.880395\n",
      "                      feature  importance\n",
      "66  device_type_register_rate       149.6\n",
      "2                 device_type       147.8\n",
      "20            act_day15_count       127.4\n",
      "54                app_day_max       125.2\n",
      "26            act_page1_count       119.2\n",
      "60             device_exp_reg       113.0\n",
      "52                app_day_std       110.0\n",
      "64                 page0_rate       109.6\n",
      "53               app_day_skew       101.0\n",
      "25            act_page3_count        91.2\n",
      "40           single_video_std        87.4\n",
      "50              app_day_count        84.6\n",
      "32             act_author_std        84.0\n",
      "22            act_day14_count        82.2\n",
      "51               app_day_mean        81.4\n",
      "62                 type1_rate        81.0\n",
      "33            act_author_mean        80.6\n",
      "39          single_video_mean        79.8\n",
      "23            act_page0_count        79.6\n",
      "61                 type0_rate        79.0\n",
      "65                 page2_rate        78.6\n",
      "1               register_type        78.2\n",
      "4                act_day_skew        76.2\n",
      "57           app_day_diff_std        73.6\n",
      "6                act_day_mean        72.4\n",
      "11            act_day13_count        71.6\n",
      "14            act_day12_count        71.2\n",
      "7                 act_day_max        70.0\n",
      "24            act_page2_count        70.0\n",
      "28            act_type0_count        66.6\n",
      "..                        ...         ...\n",
      "27            act_page4_count        44.0\n",
      "69             is_have_type12        42.4\n",
      "13             act_day3_count        39.0\n",
      "16             act_day5_count        38.4\n",
      "12             act_day4_count        37.8\n",
      "38             act_video_nums        37.0\n",
      "21             act_day2_count        36.6\n",
      "30            act_type3_count        33.2\n",
      "35               act_last_gap        33.0\n",
      "31            act_type2_count        33.0\n",
      "0                register_day        27.6\n",
      "46              video_day_max        26.8\n",
      "70         is_have_act_type12        26.8\n",
      "44             video_day_mean        23.6\n",
      "8           act_continue_mean        23.6\n",
      "68            is_have_type012        22.8\n",
      "58          app_continue_mean        22.0\n",
      "47             video_day_skew        20.2\n",
      "48           video_daily_mean        19.4\n",
      "45            video_day_count        18.8\n",
      "49            video_daily_std        15.8\n",
      "43              video_day_std        14.6\n",
      "41       every_video_type_std        13.8\n",
      "55           app_day_diff_max        13.6\n",
      "42      every_video_type_mean        13.0\n",
      "67          is_have_act_video         1.6\n",
      "72           is_have_type12_1         0.2\n",
      "71       is_have_act_type12_1         0.0\n",
      "73          is_have_type012_1         0.0\n",
      "74                       dev2         0.0\n",
      "\n",
      "[75 rows x 2 columns]\n",
      "['device_type_register_rate', 'device_type', 'act_day15_count', 'app_day_max', 'act_page1_count', 'device_exp_reg', 'app_day_std', 'page0_rate', 'app_day_skew', 'act_page3_count', 'single_video_std', 'app_day_count', 'act_author_std', 'act_day14_count', 'app_day_mean', 'type1_rate', 'act_author_mean', 'single_video_mean', 'act_page0_count', 'type0_rate', 'page2_rate', 'register_type', 'act_day_skew', 'app_day_diff_std', 'act_day_mean', 'act_day13_count', 'act_day12_count', 'act_day_max', 'act_page2_count', 'act_type0_count', 'act_day11_count', 'act_daily_std', 'type2_rate', 'act_day8_count', 'act_day7_count', 'act_day10_count', 'act_author_count', 'act_day_std', 'act_daily_mean', 'act_day9_count', 'app_day_diff_mean', 'app_last_gap', 'act_type1_count', 'act_day6_count', 'act_day_count', 'act_page4_count', 'is_have_type12', 'act_day3_count', 'act_day5_count', 'act_day4_count', 'act_video_nums', 'act_day2_count', 'act_type3_count', 'act_last_gap', 'act_type2_count', 'register_day', 'video_day_max', 'is_have_act_type12', 'video_day_mean', 'act_continue_mean', 'is_have_type012', 'app_continue_mean', 'video_day_skew', 'video_daily_mean', 'video_day_count', 'video_daily_std', 'video_day_std', 'every_video_type_std', 'app_day_diff_max', 'every_video_type_mean', 'is_have_act_video', 'is_have_type12_1', 'is_have_act_type12_1', 'is_have_type012_1', 'dev2']\n",
      "xx_cv 0.8821964522308386\n",
      "线下验证分数 0.8895070743236058\n",
      "time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "n1 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/nn1.csv\")\n",
    "n1 = rate_fun(n1)\n",
    "\n",
    "n2 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/nn2.csv\")\n",
    "n2 = rate_fun(n2)\n",
    "\n",
    "n = pd.concat([n1,n2],axis=0,ignore_index=True,join='outer')\n",
    "\n",
    "n3 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/nn3.csv\")\n",
    "n3 = rate_fun(n3)\n",
    "train_data(n1,n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "6CA1F24FE92C495B91D9ED932ED15F5D",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_id', 'register_day', 'register_type', 'device_type', 'act_video_nums']\n",
      "time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "f1 = pd.read_csv(\"/home/kesci/bigdata/train1/fuck1_feature.csv\")\n",
    "n1 = pd.read_csv(\"/home/kesci/input/home/kesci/bigdata/train1/nn1_feature.csv\")\n",
    "n_col = n1.columns.values.tolist()\n",
    "f_col = f1.columns.values.tolist()\n",
    "df = []\n",
    "for i in n_col:\n",
    "    if i in f_col:\n",
    "        df.append(i)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "BB9507CD4EAA40918CAD7DE14DC73811",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.26 ms\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # 用于特征的标准化\n",
    "from sklearn.preprocessing import Imputer\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "0D629B8584714D778079D19EECB86522",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.9 ms\n"
     ]
    }
   ],
   "source": [
    "def get_rate(data):\n",
    "    data['type0_rate'] = data['act_type0_count']/(data['act_day_count']+0.00001)\n",
    "    data['type1_rate'] = data['act_type1_count']/(data['act_day_count']+0.00001)\n",
    "    data['type2_rate'] = data['act_type2_count']/(data['act_day_count']+0.00001)\n",
    "    data['type3_rate'] = data['act_type3_count']/(data['act_day_count']+0.00001)\n",
    "    \n",
    "    data['page0_rate'] = data['act_page0_count']/(data['act_day_count']+0.00001)\n",
    "    data['page1_rate'] = data['act_page1_count']/(data['act_day_count']+0.00001)\n",
    "    data['page2_rate'] = data['act_page2_count']/(data['act_day_count']+0.00001)\n",
    "    data['page3_rate'] = data['act_page3_count']/(data['act_day_count']+0.00001)\n",
    "    data['page4_rate'] = data['act_page4_count']/(data['act_day_count']+0.00001)\n",
    "    \n",
    "    act_mean = data['act_day_count'].mean()\n",
    "    data['is_have_act'] = data['act_day_count'].apply(lambda x:0 if x<=0 else (1 if x < act_mean else 2))\n",
    "    video_mean = data['video_day_count'].mean()\n",
    "    data['is_have_video'] = data['video_day_count'].apply(lambda x:0 if x<=0 else (1 if x < video_mean else 2))\n",
    "    type0_mean = data['act_type0_count'].mean()\n",
    "    data['is_have_type0'] = data['act_type0_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < type0_mean else 2))\n",
    "    type1_mean = data['act_type1_count'].mean()\n",
    "    data['is_have_type1'] = data['act_type1_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < type1_mean else 2))\n",
    "    type2_mean = data['act_type2_count'].mean()\n",
    "    data['is_have_type2'] = data['act_type2_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < type2_mean else 2))\n",
    "    \n",
    "    act_mean = data['act_day_count'].mean()\n",
    "    data['act_day_count1'] = data['act_day_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < act_mean else 2))\n",
    "    \n",
    "    act_max = data['act_day_max'].mean()\n",
    "    data['act_day_max1'] = data['act_day_max'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < act_max else 2))\n",
    "    data['act_day_max2'] = data['act_day_max'].fillna(0).apply(lambda x:0 if x<=0 else 1)\n",
    "    page2_mean = data['act_page2_count'].mean()\n",
    "    data['is_have_page2'] = data['act_page2_count'].fillna(0).apply(lambda x:0 if x<=0 else (1 if x < page2_mean else 2))\n",
    "    data['every_video_rate'] = data['act_day_count']/(data['act_video_nums']+0.00001)\n",
    "    data['every_video_author_rate'] = data['act_author_counts']/(data['act_video_nums']+0.00001)\n",
    "    # data['author_act_rate'] =data['act_author_counts']/data['act_day_count']\n",
    "    \n",
    "    data['act_daily_max_rate'] = data['act_daily_max']/(data['act_day_count']+0.00001)\n",
    "    data['act_single_video_rate'] = data['single_video_max']/(data['act_day_count']+0.00001)\n",
    "    # data['app_daily_max_rate'] = data['app_daily_max']/(data['app_day_count']+0.00001)\n",
    "    \n",
    "    # data['every_page_type_rate'] = data['every_page_type_max']/data['act_daily_max']\n",
    "    # data['every_video_type_rate'] = data['every_video_type_max']/data['act_daily_max']\n",
    "    \n",
    "    data['app_act_rate'] = data['app_day_std']/(data['act_last1days_counts']+0.00001)\n",
    "    data['app_video_type_rate'] = data['app_day_max']/(data['every_video_type_mean']+0.00001)\n",
    "    data['app_video_type_sub'] = data['app_day_max']-data['every_video_type_mean']\n",
    "    \n",
    "    data['page1_2_rate'] = data['page2_rate']/(data['page1_rate']+0.00001)\n",
    "    data['app_std_mul'] = data['app_day_std']*data['page1_rate']\n",
    "    data['act_last1_mul'] = data['act_last1days_counts']*data['page1_rate']\n",
    "    data['act_last1_std_mul'] = (data['act_last1days_counts']+data['app_day_std'])*data['page1_rate']\n",
    "    \n",
    "    data['app_diff_rate'] = data['app_day_diff_std']/(data['app_day_std']+0.00001)\n",
    "    data['app_page1_rate'] = data['app_day_diff_std']*data['page1_rate']\n",
    "    data['page1_type2_rate'] = data['page1_rate']*data['type2_rate']\n",
    "    \n",
    "    data['app_std_div_mean'] = data['app_day_std']/(data['app_day_mean']+0.00001)\n",
    "    data['act_std_div_mean'] = data['act_day_std']/(data['act_day_mean']+0.00001)\n",
    "    data['video_std_div_mean'] = data['video_day_std']/(data['video_day_mean']+0.00001)\n",
    "    \n",
    "    data['register_day'] = data['register_day'] + 1\n",
    "    # data['register_day'] = data['register_day'].apply(lambda x:16 if x>16 else x)\n",
    "    \n",
    "    data['device_type1'] = data['device_type'].apply(lambda x:0 if x< 85 else (1 if x<170 else 2))\n",
    "    \n",
    "    data['act_continue_std_div_mean'] = data['act_continue_std']/(data['act_continue_mean']+0.00001)\n",
    "    data['app_continue_std_div_mean'] = data['app_continue_std']/(data['app_continue_mean']+0.00001)\n",
    "    \n",
    "    data['app_div_reg'] = data['app_day_count']/data['register_day']\n",
    "    data['act_div_reg'] = data['act_day_count']/data['register_day']\n",
    "    data['video_div_reg'] = data['video_day_count']/data['register_day']\n",
    "    data['author_div_reg'] = data['act_author_count']/data['register_day']\n",
    "    data['act_video_div_reg'] = data['act_video_nums']/data['register_day']\n",
    "    \n",
    "    data['act_last1days_div_reg'] = data['act_last1days_counts']/data['register_day']\n",
    "    data['device_exp_reg'] = data['device_type']/(data['register_type']+0.00001)\n",
    "    \n",
    "    data['video_max_min'] = data['video_day_max']-data['video_day_min']\n",
    "    data['app_max_min'] = data['app_day_max']-data['app_day_min']\n",
    "    data['act_max_min'] = data['act_day_max']-data['act_day_min']\n",
    "    \n",
    "    data['act_app_day_exp'] = data['app_day_max']/(data['act_day_max']+0.00001)\n",
    "    data['video_app_day_exp'] = data['app_day_max']/(data['video_day_max']+0.00001)\n",
    "    \n",
    "    data['app_day_diff_std_mean'] = data['app_day_diff_std']/(data['app_day_diff_mean']+0.00001)\n",
    "    data['app_day_std_mean'] = data['app_day_std']/(data['app_day_mean']+0.00001)\n",
    "    \n",
    "    data['act_app_day_mul'] = data['app_day_max']+data['act_day_max']\n",
    "    data['video_app_day_mul'] = data['app_day_max']+data['video_day_max']\n",
    "    data['video_act_day_mul'] = data['act_day_max']+data['video_day_max']\n",
    "    \n",
    "    data['author_rate'] = data['act_author_count']/data['act_author_counts']\n",
    "    \n",
    "    data['act_author_rate'] = data['act_author_counts']/data['act_day_count']\n",
    "    data['app_author_rate'] = data['act_author_counts']/data['app_day_count']\n",
    "    data['video_author_rate'] = data['act_author_counts']/data['video_day_count']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "62501416BF4A4E2B9B246136B5897F6B",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.3 ms\n"
     ]
    }
   ],
   "source": [
    "def auc(y_true, y_pred):  \n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)  \n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)  \n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)  \n",
    "    binSizes = -(pfas[1:]-pfas[:-1])  \n",
    "    s = ptas*binSizes  \n",
    "    return K.sum(s, axis=0)  \n",
    "\n",
    "# PFA, prob false alert for binary classifier  \n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):  \n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')  \n",
    "    # N = total number of negative labels  \n",
    "    N = K.sum(1 - y_true)  \n",
    "    # FP = total number of false alerts, alerts from the negative class labels  \n",
    "    FP = K.sum(y_pred - y_pred * y_true)  \n",
    "    return FP/N  \n",
    "\n",
    "# P_TA prob true alerts for binary classifier  \n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):  \n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')  \n",
    "    # P = total number of positive labels  \n",
    "    P = K.sum(y_true)  \n",
    "    # TP = total number of correct alerts, alerts from the positive class labels  \n",
    "    TP = K.sum(y_pred * y_true)  \n",
    "    return TP/P \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "17306DE4A0054F209912BC41950D7625",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.64 ms\n"
     ]
    }
   ],
   "source": [
    "def simple_keras(train_df,train_y,val_df,val_y,test_df):\n",
    "    '''\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imp.fit(train_df)\n",
    "    train_df = imp.transform(train_df)\n",
    "    val_df = imp.transform(val_df)\n",
    "    test_df = imp.transform(test_df)\n",
    "    '''\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(train_df)\n",
    "    train_df = sc.transform(train_df)\n",
    "    val_df = sc.transform(val_df)\n",
    "    test_df = sc.transform(test_df)\n",
    "    \n",
    "    \n",
    "    # model = Sequential()\n",
    "    # model.add(Dense(64, input_shape=(train_df.shape[1],)))\n",
    "    # model.add(Activation('tanh'))\n",
    "    # model.add(Dropout(0.3))\n",
    "    # model.add(Dense(64))\n",
    "    # model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.3))\n",
    "    # model.add(Dense(64))\n",
    "    # model.add(Activation('tanh'))\n",
    "    # model.add(Dropout(0.3))\n",
    "    # model.add(Dense(64))\n",
    "    # model.add(Activation('linear'))\n",
    "    # model.add(Dense(1)) # 这里需要和输出的维度一致\n",
    "    # model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model = get_model(train_df)\n",
    "    \n",
    "    # For a multi-class classification problem\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='nadam',\n",
    "                  #metrics=['accuracy']\n",
    "                  metrics=[auc]\n",
    "                  )\n",
    "    # model.load_weights(\"/home/kesci/bigdata/model/nn_model_2018-07-27 14:14:04.h5\")\n",
    "    time_date = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))\n",
    "    filepath = '/home/kesci/bigdata/model/10fold_mlp_param_%s.h5'%(str(time_date))\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    model.fit(train_df,train_y,epochs=10,batch_size=1024,\n",
    "                    #validation_data=[[X_test,X_test1],y_test],\n",
    "                    validation_data=[val_df,val_y],\n",
    "                    shuffle=True,verbose=2,\n",
    "                    # callbacks = [EarlyStopping(monitor='val_auc',patience=5,mode='max'),checkpoint]\n",
    "                    )\n",
    "\n",
    "    # time_date = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))\n",
    "    model.save_weights(\"/home/kesci/bigdata/model/nn_model_%s.h5\"%(str(time_date)))\n",
    "    test_y = model.predict(test_df)\n",
    "    y_pred = model.predict(val_df)\n",
    "    \n",
    "\n",
    "    return y_pred,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "C3A8366944414CF997C015253AE61505",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.4 ms\n"
     ]
    }
   ],
   "source": [
    "def get_model_nn1():\n",
    "    xin = Input(shape=(X.shape[1],))\n",
    "    xin1 = Input(shape=(X1.shape[1],X1.shape[2]))\n",
    "    \n",
    "    x1 = Dense(32,init='he_normal')(xin)\n",
    "    x1 = PReLU()(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1b = x1\n",
    "\n",
    "    x2 = LSTM(36,activation='relu',dropout=0.2,return_sequences=False)(xin1)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2b = x2\n",
    "\n",
    "    x3 = Conv1D(20,3)(xin1)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = Activation('relu')(x3)\n",
    "    x3 = MaxPooling1D(2)(x3)\n",
    "    x3 = LSTM(10,activation='relu',dropout=0.2,return_sequences=False)(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3b = x3\n",
    "    \n",
    "    xb = Concatenate(axis=1)([x1b,x2b,x3b])\n",
    "    xb = Dense(64,init='he_normal')(xb)\n",
    "    xb = PReLU()(xb)\n",
    "    xb = BatchNormalization()(xb)\n",
    "    xb = Dropout(0.25)(xb)\n",
    "    xb = Dense(16,init='he_normal')(xb)\n",
    "    xb = Dropout(0.5)(xb)\n",
    "    y = Dense(1, activation='sigmoid')(xb)\n",
    "    \n",
    "    model = Model(inputs=[xin,xin1],outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "C1DD812DB2784CE1887E00B418F4915E",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.84 ms\n"
     ]
    }
   ],
   "source": [
    "def get_model(X):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(X.shape[1],),init='he_normal'))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(64,init='he_normal'))\n",
    "    \n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        model.add(Dense(34,init='he_normal'))\n",
    "    \n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(128,init='he_normal'))\n",
    "        '''\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "     \n",
    "        model.add(Dense(16,init='he_normal'))\n",
    "        '''\n",
    "        \n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(1)) \n",
    "        model.add(Activation('sigmoid'))\n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "D71D7F13D9D44289A17FBC745C6C7256",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.4 ms\n"
     ]
    }
   ],
   "source": [
    "def model_train(t,test):\n",
    "    \n",
    "    t = t.fillna(0)\n",
    "    # t = t.astype(\"float32\")\n",
    "    test = test.fillna(0)\n",
    "    t_userid = t.pop('user_id')\n",
    "    y = t.pop('label')\n",
    "    col = [\n",
    "        'register_day', 'register_type', 'device_type', 'device_type_register_count', 'device_type_count', 'device_type_register_rate', 'act_day_skew', 'act_day_sum', 'act_day_kurt', 'act_day_mean', 'act_day_max', 'act_day_min', 'act_day_std', 'act_day_count', 'act_day_var', 'act_last1days_counts', 'act_last2days_counts', 'act_last3days_counts', 'act_last4days_counts', 'act_last5days_counts', 'act_last6days_counts', 'act_last7days_counts', 'act_nums', 'author_nums', 'act_video_nums', 'action_type_nums', 'page_nums', 'act_continue_std', 'act_continue_mean', 'act_last_gap', 'act_page0_count', 'act_page2_count', 'act_page3_count', 'act_page1_count', 'act_page4_count', 'act_type0_count', 'act_type1_count', 'act_type3_count', 'act_type2_count', 'act_type4_count', 'act_type5_count', 'act_author_counts', 'act_author_std', 'act_author_mean', 'act_author_count', 'act_daily_max', 'single_video_max', 'every_video_type_mean', 'every_page_type_mean', 'video_day_var', 'video_day_count', 'video_day_min', 'video_day_std', 'video_day_mean', 'video_day_skew', 'video_day_sum', 'video_day_max', 'video_day_kurt', 'video_nums', 'video_last7days_counts', 'app_day_min', 'app_day_std', 'app_day_skew', 'app_day_mean', 'app_day_max', 'app_day_kurt', 'app_day_var', 'app_day_count', 'app_day_sum', 'app_day_diff_min', 'app_day_diff_kurt', 'app_day_diff_var', 'app_day_diff_sum', 'app_day_diff_std', 'app_day_diff_count', 'app_day_diff_skew', 'app_day_diff_max', 'app_day_diff_mean', 'app_nums', 'app_last_gap', 'app_continue_std', 'app_continue_mean', 'app_last1days_counts', 'app_last2days_counts', 'app_last3days_counts', 'app_last4days_counts', 'app_last5days_counts', 'app_last6days_counts', 'app_last7days_counts', 'type0_rate', 'type1_rate', 'type2_rate', 'type3_rate', 'page0_rate', 'page1_rate', 'page2_rate', 'page3_rate', 'page4_rate', 'is_have_act', 'is_have_video', 'is_have_type0', 'is_have_type1', 'is_have_type2', 'act_day_count1', 'act_day_max1', 'act_day_max2', 'every_video_rate', 'every_video_author_rate', 'act_daily_max_rate', 'act_single_video_rate', 'app_act_rate', 'app_video_type_rate', 'app_video_type_sub', 'page1_2_rate', 'app_std_mul', 'act_last1_mul', 'act_last1_std_mul', 'app_diff_rate', 'app_page1_rate', 'page1_type2_rate', 'app_std_div_mean', 'act_std_div_mean', 'video_std_div_mean', 'device_type1', 'act_continue_std_div_mean', 'app_continue_std_div_mean', 'app_div_reg', 'act_div_reg', 'video_div_reg', 'author_div_reg', 'act_video_div_reg', 'act_last1days_div_reg', 'device_exp_reg', 'video_max_min', 'app_max_min', 'act_max_min', 'act_app_day_exp', 'video_app_day_exp', 'app_day_diff_std_mean', 'app_day_std_mean', 'act_app_day_mul', 'video_app_day_mul', 'video_act_day_mul', 'author_rate', 'act_author_rate', 'app_author_rate', 'video_author_rate'\n",
    "        ]\n",
    "    X = t[col].values\n",
    "    print(len(col))\n",
    "    \n",
    "    # test_label = test['label']\n",
    "    test_userid = test.pop(\"user_id\")\n",
    "    test = test[col].values\n",
    "    \n",
    "    N = 10\n",
    "    skf = StratifiedKFold(n_splits=N,shuffle=True,random_state=8888)\n",
    "    \n",
    "    xx_cv = []\n",
    "    xx_pre = []\n",
    "    i=1\n",
    "    for train_in,test_in in skf.split(X,y):\n",
    "        print(\"折数\",i)\n",
    "        X_train,X_test,y_train,y_test = X[train_in],X[test_in],y[train_in],y[test_in]\n",
    "    \n",
    "        y_pred, ty = simple_keras(X_train,y_train,X_test,y_test,test)\n",
    "        xx_pre.append(ty)\n",
    "        xx_cv.append(roc_auc_score(y_test,y_pred))\n",
    "        print (roc_auc_score(y_test,y_pred))\n",
    "        # print (\"线下验证--》\",roc_auc_score(test_label,ty))\n",
    "        i = i+1\n",
    "    \n",
    "    s = 0\n",
    "    for i in xx_pre:\n",
    "        s = s + i\n",
    "    s = s /N\n",
    "    res = pd.DataFrame()\n",
    "    res['user_id'] = list(test_userid.values)\n",
    "    res['RST'] = list(s)\n",
    "    # res['RST'] = res['RST'][:,0]\n",
    "    print('xx_cv',np.mean(xx_cv))\n",
    "    time_date = time.strftime('%Y-%m-%d',time.localtime(time.time()))\n",
    "    res.to_csv('/home/kesci/bigdata/result/nn_30epoch_%s_%s.csv'%(str(time_date),str(np.mean(xx_cv)).split('.')[1]),sep=',',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "0D3193C4ECBF4029A66EE16755953CE4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "折数 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, kernel_initializer=\"he_normal\", input_shape=(147,))`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, kernel_initializer=\"he_normal\")`\n",
      "  \n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(34, kernel_initializer=\"he_normal\")`\n",
      "  \n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, kernel_initializer=\"he_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 610419 samples, validate on 67825 samples\n",
      "Epoch 1/10\n",
      " - 95s - loss: 0.4448 - auc: 0.8749 - val_loss: 0.4278 - val_auc: 0.8752\n",
      "Epoch 2/10\n",
      " - 32s - loss: 0.4299 - auc: 0.8827 - val_loss: 0.4259 - val_auc: 0.8759\n",
      "Epoch 3/10\n",
      " - 31s - loss: 0.4279 - auc: 0.8839 - val_loss: 0.4252 - val_auc: 0.8763\n",
      "Epoch 4/10\n",
      " - 31s - loss: 0.4262 - auc: 0.8848 - val_loss: 0.4259 - val_auc: 0.8765\n",
      "Epoch 5/10\n",
      " - 31s - loss: 0.4255 - auc: 0.8852 - val_loss: 0.4247 - val_auc: 0.8768\n",
      "Epoch 6/10\n",
      " - 30s - loss: 0.4248 - auc: 0.8854 - val_loss: 0.4236 - val_auc: 0.8772\n",
      "Epoch 7/10\n",
      " - 31s - loss: 0.4238 - auc: 0.8860 - val_loss: 0.4253 - val_auc: 0.8772\n",
      "Epoch 8/10\n",
      " - 31s - loss: 0.4233 - auc: 0.8863 - val_loss: 0.4239 - val_auc: 0.8775\n",
      "Epoch 9/10\n",
      " - 29s - loss: 0.4229 - auc: 0.8865 - val_loss: 0.4238 - val_auc: 0.8774\n",
      "Epoch 10/10\n",
      " - 31s - loss: 0.4227 - auc: 0.8867 - val_loss: 0.4245 - val_auc: 0.8775\n",
      "0.8858335748734163\n",
      "折数 2\n",
      "Train on 610419 samples, validate on 67825 samples\n",
      "Epoch 1/10\n",
      " - 98s - loss: 0.4449 - auc: 0.8746 - val_loss: 0.4219 - val_auc: 0.8780\n",
      "Epoch 2/10\n",
      " - 30s - loss: 0.4300 - auc: 0.8827 - val_loss: 0.4216 - val_auc: 0.8785\n",
      "Epoch 3/10\n",
      " - 31s - loss: 0.4279 - auc: 0.8838 - val_loss: 0.4199 - val_auc: 0.8791\n",
      "Epoch 4/10\n",
      " - 30s - loss: 0.4267 - auc: 0.8844 - val_loss: 0.4195 - val_auc: 0.8794\n",
      "Epoch 5/10\n",
      " - 32s - loss: 0.4260 - auc: 0.8849 - val_loss: 0.4195 - val_auc: 0.8796\n",
      "Epoch 6/10\n",
      " - 31s - loss: 0.4251 - auc: 0.8854 - val_loss: 0.4189 - val_auc: 0.8797\n",
      "Epoch 7/10\n",
      " - 31s - loss: 0.4244 - auc: 0.8856 - val_loss: 0.4182 - val_auc: 0.8803\n",
      "Epoch 8/10\n",
      " - 31s - loss: 0.4243 - auc: 0.8858 - val_loss: 0.4184 - val_auc: 0.8800\n",
      "Epoch 9/10\n",
      " - 31s - loss: 0.4238 - auc: 0.8860 - val_loss: 0.4184 - val_auc: 0.8803\n",
      "Epoch 10/10\n",
      " - 31s - loss: 0.4230 - auc: 0.8865 - val_loss: 0.4181 - val_auc: 0.8805\n",
      "0.8890533493520792\n",
      "折数 3\n",
      "Train on 610419 samples, validate on 67825 samples\n",
      "Epoch 1/10\n",
      " - 99s - loss: 0.4454 - auc: 0.8746 - val_loss: 0.4178 - val_auc: 0.8805\n",
      "Epoch 2/10\n",
      " - 31s - loss: 0.4307 - auc: 0.8823 - val_loss: 0.4170 - val_auc: 0.8810\n",
      "Epoch 3/10\n",
      " - 31s - loss: 0.4285 - auc: 0.8835 - val_loss: 0.4162 - val_auc: 0.8815\n",
      "Epoch 4/10\n",
      " - 31s - loss: 0.4274 - auc: 0.8842 - val_loss: 0.4157 - val_auc: 0.8815\n",
      "Epoch 5/10\n",
      " - 30s - loss: 0.4265 - auc: 0.8846 - val_loss: 0.4153 - val_auc: 0.8821\n",
      "Epoch 6/10\n",
      " - 30s - loss: 0.4258 - auc: 0.8850 - val_loss: 0.4142 - val_auc: 0.8821\n",
      "Epoch 7/10\n",
      " - 31s - loss: 0.4252 - auc: 0.8852 - val_loss: 0.4143 - val_auc: 0.8823\n",
      "Epoch 8/10\n",
      " - 32s - loss: 0.4243 - auc: 0.8857 - val_loss: 0.4145 - val_auc: 0.8823\n",
      "Epoch 9/10\n",
      " - 32s - loss: 0.4242 - auc: 0.8857 - val_loss: 0.4142 - val_auc: 0.8826\n",
      "Epoch 10/10\n",
      " - 34s - loss: 0.4238 - auc: 0.8861 - val_loss: 0.4140 - val_auc: 0.8826\n",
      "0.8915633167531372\n",
      "折数 4\n",
      "Train on 610419 samples, validate on 67825 samples\n",
      "Epoch 1/10\n",
      " - 102s - loss: 0.4444 - auc: 0.8750 - val_loss: 0.4244 - val_auc: 0.8769\n",
      "Epoch 2/10\n",
      " - 31s - loss: 0.4297 - auc: 0.8828 - val_loss: 0.4241 - val_auc: 0.8772\n",
      "Epoch 3/10\n",
      " - 31s - loss: 0.4277 - auc: 0.8839 - val_loss: 0.4223 - val_auc: 0.8776\n",
      "Epoch 4/10\n",
      " - 31s - loss: 0.4267 - auc: 0.8846 - val_loss: 0.4226 - val_auc: 0.8780\n",
      "Epoch 5/10\n",
      " - 31s - loss: 0.4257 - auc: 0.8850 - val_loss: 0.4221 - val_auc: 0.8780\n",
      "Epoch 6/10\n",
      " - 31s - loss: 0.4249 - auc: 0.8854 - val_loss: 0.4228 - val_auc: 0.8781\n",
      "Epoch 7/10\n",
      " - 31s - loss: 0.4242 - auc: 0.8858 - val_loss: 0.4220 - val_auc: 0.8783\n",
      "Epoch 8/10\n",
      " - 31s - loss: 0.4241 - auc: 0.8860 - val_loss: 0.4212 - val_auc: 0.8785\n",
      "Epoch 9/10\n",
      " - 31s - loss: 0.4233 - auc: 0.8862 - val_loss: 0.4211 - val_auc: 0.8786\n",
      "Epoch 10/10\n",
      " - 30s - loss: 0.4230 - auc: 0.8865 - val_loss: 0.4222 - val_auc: 0.8788\n",
      "0.8871544991861738\n",
      "折数 5\n",
      "Train on 610420 samples, validate on 67824 samples\n",
      "Epoch 1/10\n",
      " - 103s - loss: 0.4480 - auc: 0.8733 - val_loss: 0.4228 - val_auc: 0.8777\n",
      "Epoch 2/10\n",
      " - 32s - loss: 0.4301 - auc: 0.8826 - val_loss: 0.4215 - val_auc: 0.8785\n",
      "Epoch 3/10\n",
      " - 32s - loss: 0.4282 - auc: 0.8837 - val_loss: 0.4204 - val_auc: 0.8790\n",
      "Epoch 4/10\n",
      " - 31s - loss: 0.4270 - auc: 0.8843 - val_loss: 0.4203 - val_auc: 0.8791\n",
      "Epoch 5/10\n",
      " - 31s - loss: 0.4259 - auc: 0.8849 - val_loss: 0.4244 - val_auc: 0.8791\n",
      "Epoch 6/10\n",
      " - 31s - loss: 0.4253 - auc: 0.8852 - val_loss: 0.4187 - val_auc: 0.8798\n",
      "Epoch 7/10\n",
      " - 30s - loss: 0.4246 - auc: 0.8856 - val_loss: 0.4185 - val_auc: 0.8801\n",
      "Epoch 8/10\n",
      " - 30s - loss: 0.4242 - auc: 0.8859 - val_loss: 0.4188 - val_auc: 0.8800\n",
      "Epoch 9/10\n",
      " - 31s - loss: 0.4236 - auc: 0.8861 - val_loss: 0.4181 - val_auc: 0.8800\n",
      "Epoch 10/10\n",
      " - 30s - loss: 0.4233 - auc: 0.8863 - val_loss: 0.4177 - val_auc: 0.8802\n",
      "0.8886565500908954\n",
      "折数 6\n",
      "Train on 610420 samples, validate on 67824 samples\n",
      "Epoch 1/10\n",
      " - 105s - loss: 0.4445 - auc: 0.8751 - val_loss: 0.4246 - val_auc: 0.8769\n",
      "Epoch 2/10\n",
      " - 31s - loss: 0.4296 - auc: 0.8828 - val_loss: 0.4245 - val_auc: 0.8772\n",
      "Epoch 3/10\n",
      " - 32s - loss: 0.4279 - auc: 0.8838 - val_loss: 0.4216 - val_auc: 0.8780\n",
      "Epoch 4/10\n",
      " - 30s - loss: 0.4262 - auc: 0.8848 - val_loss: 0.4218 - val_auc: 0.8779\n",
      "Epoch 5/10\n",
      " - 31s - loss: 0.4258 - auc: 0.8850 - val_loss: 0.4218 - val_auc: 0.8779\n",
      "Epoch 6/10\n",
      " - 32s - loss: 0.4249 - auc: 0.8855 - val_loss: 0.4210 - val_auc: 0.8783\n",
      "Epoch 7/10\n",
      " - 32s - loss: 0.4245 - auc: 0.8857 - val_loss: 0.4216 - val_auc: 0.8785\n",
      "Epoch 8/10\n",
      " - 31s - loss: 0.4241 - auc: 0.8860 - val_loss: 0.4206 - val_auc: 0.8788\n",
      "Epoch 9/10\n",
      " - 32s - loss: 0.4238 - auc: 0.8861 - val_loss: 0.4208 - val_auc: 0.8785\n",
      "Epoch 10/10\n",
      " - 30s - loss: 0.4231 - auc: 0.8864 - val_loss: 0.4201 - val_auc: 0.8788\n",
      "0.8875738293081563\n",
      "折数 7\n",
      "Train on 610420 samples, validate on 67824 samples\n",
      "Epoch 1/10\n",
      " - 107s - loss: 0.4449 - auc: 0.8752 - val_loss: 0.4248 - val_auc: 0.8759\n",
      "Epoch 2/10\n",
      " - 31s - loss: 0.4299 - auc: 0.8828 - val_loss: 0.4232 - val_auc: 0.8765\n",
      "Epoch 3/10\n",
      " - 31s - loss: 0.4277 - auc: 0.8840 - val_loss: 0.4233 - val_auc: 0.8766\n",
      "Epoch 4/10\n",
      " - 31s - loss: 0.4264 - auc: 0.8847 - val_loss: 0.4222 - val_auc: 0.8772\n",
      "Epoch 5/10\n",
      " - 31s - loss: 0.4257 - auc: 0.8851 - val_loss: 0.4231 - val_auc: 0.8773\n",
      "Epoch 6/10\n",
      " - 33s - loss: 0.4248 - auc: 0.8855 - val_loss: 0.4222 - val_auc: 0.8776\n",
      "Epoch 7/10\n",
      " - 31s - loss: 0.4242 - auc: 0.8858 - val_loss: 0.4212 - val_auc: 0.8776\n",
      "Epoch 8/10\n",
      " - 31s - loss: 0.4237 - auc: 0.8861 - val_loss: 0.4209 - val_auc: 0.8780\n",
      "Epoch 9/10\n",
      " - 31s - loss: 0.4234 - auc: 0.8864 - val_loss: 0.4209 - val_auc: 0.8780\n",
      "Epoch 10/10\n",
      " - 32s - loss: 0.4230 - auc: 0.8865 - val_loss: 0.4206 - val_auc: 0.8781\n",
      "0.8872025212900754\n",
      "折数 8\n",
      "Train on 610420 samples, validate on 67824 samples\n",
      "Epoch 1/10\n",
      " - 107s - loss: 0.4448 - auc: 0.8747 - val_loss: 0.4246 - val_auc: 0.8768\n",
      "Epoch 2/10\n",
      " - 30s - loss: 0.4297 - auc: 0.8828 - val_loss: 0.4228 - val_auc: 0.8775\n",
      "Epoch 3/10\n",
      " - 30s - loss: 0.4280 - auc: 0.8838 - val_loss: 0.4224 - val_auc: 0.8779\n",
      "Epoch 4/10\n",
      " - 30s - loss: 0.4266 - auc: 0.8845 - val_loss: 0.4218 - val_auc: 0.8779\n",
      "Epoch 5/10\n",
      " - 30s - loss: 0.4260 - auc: 0.8850 - val_loss: 0.4221 - val_auc: 0.8784\n",
      "Epoch 6/10\n",
      " - 30s - loss: 0.4251 - auc: 0.8853 - val_loss: 0.4214 - val_auc: 0.8785\n",
      "Epoch 7/10\n",
      " - 29s - loss: 0.4245 - auc: 0.8856 - val_loss: 0.4213 - val_auc: 0.8787\n",
      "Epoch 8/10\n",
      " - 30s - loss: 0.4241 - auc: 0.8857 - val_loss: 0.4203 - val_auc: 0.8791\n",
      "Epoch 9/10\n",
      " - 30s - loss: 0.4233 - auc: 0.8862 - val_loss: 0.4202 - val_auc: 0.8791\n",
      "Epoch 10/10\n",
      " - 31s - loss: 0.4230 - auc: 0.8864 - val_loss: 0.4207 - val_auc: 0.8789\n",
      "0.8877161915985791\n",
      "折数 9\n",
      "Train on 610420 samples, validate on 67824 samples\n",
      "Epoch 1/10\n",
      " - 110s - loss: 0.4444 - auc: 0.8749 - val_loss: 0.4264 - val_auc: 0.8752\n",
      "Epoch 2/10\n",
      " - 31s - loss: 0.4296 - auc: 0.8830 - val_loss: 0.4248 - val_auc: 0.8759\n",
      "Epoch 3/10\n",
      " - 31s - loss: 0.4277 - auc: 0.8841 - val_loss: 0.4241 - val_auc: 0.8762\n",
      "Epoch 4/10\n",
      " - 31s - loss: 0.4262 - auc: 0.8849 - val_loss: 0.4248 - val_auc: 0.8764\n",
      "Epoch 5/10\n",
      " - 31s - loss: 0.4256 - auc: 0.8852 - val_loss: 0.4270 - val_auc: 0.8765\n",
      "Epoch 6/10\n",
      " - 31s - loss: 0.4248 - auc: 0.8856 - val_loss: 0.4230 - val_auc: 0.8769\n",
      "Epoch 7/10\n",
      " - 31s - loss: 0.4243 - auc: 0.8858 - val_loss: 0.4225 - val_auc: 0.8768\n",
      "Epoch 8/10\n",
      " - 31s - loss: 0.4237 - auc: 0.8861 - val_loss: 0.4222 - val_auc: 0.8771\n",
      "Epoch 9/10\n",
      " - 30s - loss: 0.4233 - auc: 0.8863 - val_loss: 0.4229 - val_auc: 0.8770\n",
      "Epoch 10/10\n",
      " - 31s - loss: 0.4231 - auc: 0.8865 - val_loss: 0.4218 - val_auc: 0.8774\n",
      "0.886316356031372\n",
      "折数 10\n",
      "Train on 610420 samples, validate on 67824 samples\n",
      "Epoch 1/10\n",
      " - 110s - loss: 0.4454 - auc: 0.8743 - val_loss: 0.4217 - val_auc: 0.8772\n",
      "Epoch 2/10\n",
      " - 33s - loss: 0.4296 - auc: 0.8828 - val_loss: 0.4204 - val_auc: 0.8777\n",
      "Epoch 3/10\n",
      " - 32s - loss: 0.4281 - auc: 0.8839 - val_loss: 0.4207 - val_auc: 0.8782\n",
      "Epoch 4/10\n",
      " - 33s - loss: 0.4270 - auc: 0.8843 - val_loss: 0.4194 - val_auc: 0.8786\n",
      "Epoch 5/10\n",
      " - 33s - loss: 0.4261 - auc: 0.8848 - val_loss: 0.4191 - val_auc: 0.8789\n",
      "Epoch 6/10\n",
      " - 32s - loss: 0.4253 - auc: 0.8853 - val_loss: 0.4197 - val_auc: 0.8792\n",
      "Epoch 7/10\n",
      " - 32s - loss: 0.4248 - auc: 0.8855 - val_loss: 0.4188 - val_auc: 0.8793\n",
      "Epoch 8/10\n",
      " - 32s - loss: 0.4242 - auc: 0.8858 - val_loss: 0.4182 - val_auc: 0.8795\n",
      "Epoch 9/10\n",
      " - 33s - loss: 0.4240 - auc: 0.8859 - val_loss: 0.4192 - val_auc: 0.8794\n",
      "Epoch 10/10\n",
      " - 32s - loss: 0.4238 - auc: 0.8860 - val_loss: 0.4186 - val_auc: 0.8799\n",
      "0.8891864009362924\n",
      "xx_cv 0.8880256589420178\n",
      "time: 1h 30min 35s\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "n1 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck1_feature.csv\")\n",
    "n1 = get_rate(n1)\n",
    "\n",
    "n2 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck2_feature.csv\")\n",
    "n2 = get_rate(n2)\n",
    "\n",
    "n = pd.concat([n1,n2],axis=0,ignore_index=True,join='outer')\n",
    "\n",
    "n3 = pd.read_csv(\"/home/kesci/input/home/kesci/fuck/fuck3_feature.csv\")\n",
    "n3 = get_rate(n3)\n",
    "# print(n1.columns.values.tolist())\n",
    "\n",
    "model_train(n,n3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "78E0C6CD4F8141B3A5648615C7056141",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "0.05 0.8895918677741311\n",
      "-----------\n",
      "0.1 0.8896298705143676\n",
      "-----------\n",
      "0.15000000000000002 0.8896646339660661\n",
      "-----------\n",
      "0.2 0.88969508868271\n",
      "-----------\n",
      "0.25 0.889721847589855\n",
      "-----------\n",
      "0.30000000000000004 0.8897449320993398\n",
      "-----------\n",
      "0.35000000000000003 0.8897644231366921\n",
      "-----------\n",
      "0.4 0.8897803038248957\n",
      "-----------\n",
      "0.45 0.8897928110499418\n",
      "-----------\n",
      "0.5 0.8898014346210222\n",
      "-----------\n",
      "0.55 0.8898062664500888\n",
      "-----------\n",
      "0.6000000000000001 0.8898080117720305\n",
      "-----------\n",
      "0.65 0.8898063389371087\n",
      "-----------\n",
      "0.7000000000000001 0.8898010119475963\n",
      "-----------\n",
      "0.75 0.8897913729738529\n",
      "-----------\n",
      "0.8 0.8897785400364122\n",
      "-----------\n",
      "0.8500000000000001 0.8897609958634349\n",
      "-----------\n",
      "0.9 0.8897402129629279\n",
      "-----------\n",
      "0.9500000000000001 0.8897165400486498\n",
      "time: 3.14 s\n"
     ]
    }
   ],
   "source": [
    "lgbtest_y = pd.read_csv(\"/home/kesci/bigdata/result/lgb_2018-08-07_8824762704486109.csv\",names=['id','res'])\n",
    "lgbtest = lgbtest_y['res']\n",
    "nnres2 = pd.read_csv(\"/home/kesci/bigdata/result/lgb_2018-08-07_8822277899293791.csv\",names=['id','res'])\n",
    "nnres = nnres2['res'].apply(lambda x: float(str(x).replace('[','').replace(']','')))\n",
    "\n",
    "train_y2 = n2['label']\n",
    "k=0.05\n",
    "h=20\n",
    "for i in range(1,h):\n",
    "    test_y2 = lgbtest*k*i+nnres*k*(h-i)\n",
    "    print(\"-----------\")\n",
    "    print (i*k,roc_auc_score(train_y2,test_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "B755BC34B9894F7E8E91889D050C597D",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.14 s\n"
     ]
    }
   ],
   "source": [
    "lgb_res = pd.read_csv(\"/home/kesci/work/lh.csv\",names=['id','res1'])\n",
    "nn_res = pd.read_csv(\"/home/kesci/bigdata/result/nn_mean.csv\",names=['id','res2'])\n",
    "res = pd.merge(lgb_res,nn_res,on=\"id\",how = 'left')\n",
    "r1 = res['res1']\n",
    "r2 = res['res2'].apply(lambda x: float(str(x).replace('[','').replace(']','')))\n",
    "# res['res2'] = res['res2'].apply(lambda x: float(str(x).replace('[','').replace(']','')))\n",
    "# print(res.corr())\n",
    "result = pd.DataFrame()\n",
    "result['id'] = res['id']\n",
    "result['res'] = r1*0.8 + r2*0.2\n",
    "time_date = time.strftime('%Y-%m-%d',time.localtime(time.time()))\n",
    "result.to_csv('/home/kesci/bigdata/result/lh_nn_%s.csv'%(str(time_date)),sep=',',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3A00BBEFDC8C434184AAF8089FECA4D4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/datasets/fusai/\"\n",
    "reg_type = {'user_id':np.uint32,'register_day':np.uint8,'register_type':np.uint8,'device_type':np.uint8}\n",
    "user_reg = pd.read_table(path+'user_register_log.txt',names=['user_id','register_day','register_type','device_type'],encoding='utf-8',dtype=reg_type,sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "98FF722E33BF43DA81CDCBB3F2E3A4AC",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.83 ms\n"
     ]
    }
   ],
   "source": [
    "out_user = user_reg[(user_reg[\"register_day\"]==24)&(user_reg[\"register_type\"]==3)&((user_reg[\"device_type\"]==1)|(user_reg[\"device_type\"]==223)|(user_reg[\"device_type\"]==83))]\n",
    "out_userid = out_user['user_id']\n",
    "# out_userid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BC031D6857BD4E5C8875C0E82A6A84D7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 174 ms\n"
     ]
    }
   ],
   "source": [
    "rhe = pd.read_csv(\"/home/kesci/bigdata/result/lh_nn_2018-08-10.csv\",names=['user_id','res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "69927044879547DE8A553123FD970B00",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    13923\n",
       "Name: res, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.78 ms\n"
     ]
    }
   ],
   "source": [
    "(rhe[rhe['res']==0])['res'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "C36F41FF02E349108B01E7FFE907A03D",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9926294068199738\n",
      "time: 9.1 s\n"
     ]
    }
   ],
   "source": [
    "lgb_res = pd.read_csv(\"/home/kesci/bigdata/result/l_x_x1_c_lr.csv\",names=['user_id','res1'])\n",
    "nn_res = pd.read_csv(\"/home/kesci/bigdata/result/nn_2018-08-10_8881918220422907.csv\",names=['user_id','res2'])\n",
    "res = pd.merge(lgb_res,nn_res,on=\"user_id\",how = 'left')\n",
    "r1 = res['res1']\n",
    "r2 = res['res2'].apply(lambda x: float(str(x).replace('[','').replace(']','')))\n",
    "# res['res2'] = res['res2'].apply(lambda x: float(str(x).replace('[','').replace(']','')))\n",
    "# print(res.corr())\n",
    "result = pd.DataFrame()\n",
    "result['user_id'] = res['user_id']\n",
    "result['res'] = r1*0.6 + r2*0.4\n",
    "\n",
    "result.to_csv('/home/kesci/bigdata/result/lh1.csv',sep=',',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "C7C81D585835415D9C460DEC967B703B",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 363 ms\n"
     ]
    }
   ],
   "source": [
    "cp /home/kesci/bigdata/result/nn_4.csv /home/kesci/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "C4C7C604361C4090A2DDF9E7F9A25933",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          user_id       lgb       xgb       ctb    xgb_lr        nn\n",
      "user_id  1.000000 -0.000199 -0.000238 -0.000273 -0.000367 -0.000535\n",
      "lgb     -0.000199  1.000000  0.999421  0.998736  0.996799  0.995085\n",
      "xgb     -0.000238  0.999421  1.000000  0.998598  0.996724  0.994983\n",
      "ctb     -0.000273  0.998736  0.998598  1.000000  0.996329  0.995459\n",
      "xgb_lr  -0.000367  0.996799  0.996724  0.996329  1.000000  0.993446\n",
      "nn      -0.000535  0.995085  0.994983  0.995459  0.993446  1.000000\n",
      "time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "lgb = pd.read_csv('/home/kesci/work/lgb_2018-08-09_8893366235403233.csv',names=['user_id','lgb'])\n",
    "xgb = pd.read_csv('/home/kesci/work/xgb_2018-08-08_889327200627967.csv',names=['user_id','xgb'])\n",
    "ctb = pd.read_csv('/home/kesci/work/cat_2018-08-09_8891670632228583.csv',names=['user_id','ctb'])\n",
    "xgb_lr = pd.read_csv('/home/kesci/bigdata/result/xgb_lr_2018-08-10.csv',names=['user_id','xgb_lr'])\n",
    "nn_res = pd.read_csv('/home/kesci/bigdata/result/nn_2018-08-10_8881918220422907.csv',names=['user_id','nn'])\n",
    "nn_res['nn'] = nn_res['nn'].apply(lambda x: float(str(x).replace('[','').replace(']','')))\n",
    "lgb = lgb.merge(xgb,on=\"user_id\",how='left')\n",
    "lgb = lgb.merge(ctb,on=\"user_id\",how='left')\n",
    "lgb = lgb.merge(xgb_lr,on=\"user_id\",how='left')\n",
    "lgb = lgb.merge(nn_res,on=\"user_id\",how='left')\n",
    "print(lgb.corr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
